# Story 3.6: Lexical Inflation Dimension

**Status:** ✅ APPROVED
**Parent Epic:** Content-Aware Analysis System (Epic 3.0) - New Dimensions
**Estimated Effort:** 2 days
**Dependencies:** Story 1.4.11 (Dimension Registry)
**Priority:** HIGH (Research-proven AI marker: 26% harder vocabulary)
**Target Version:** 1.5.0

---

## Story

As a **technical writer analyzing document quality**,
I want **the analyzer to detect lexical inflation** (AI's tendency to use 26% more complex/rare words than humans),
so that **I can identify AI-generated content through vocabulary complexity patterns**.

---

## Acceptance Criteria

### AC1: Rare Word Detection
- [ ] Uses word frequency corpus (e.g., Google Books, COCA) to classify words as rare/common
- [ ] Rare words: frequency rank >10,000 (top 10k = common)
- [ ] Calculates rare word density: `rare_words / total_words × 1000`
- [ ] Compares against human baseline (research: AI uses 26% more rare words)

### AC2: Complexity Scoring
- [ ] Syllable count: >3 syllables = complex
- [ ] Word length: >10 characters = complex
- [ ] Latinate roots: detects words from Latin/Greek (e.g., utilize, facilitate, implement)
- [ ] Calculates complexity inflation ratio vs expected baseline

### AC3: Lexical Inflation Metric
- [ ] Inflation score: `(ai_rare_word_density - human_baseline) / human_baseline`
- [ ] Baseline: 15 rare words per 1k (human average)
- [ ] AI typical: 19 rare words per 1k (26% inflation)
- [ ] EXCELLENT: inflation <10% (14-16.5 rare words/1k)
- [ ] GOOD: inflation 10-20% (16.5-18 rare words/1k)
- [ ] NEEDS WORK: inflation 20-30% (18-19.5 rare words/1k)
- [ ] POOR: inflation >30% (>19.5 rare words/1k, AI-like)

### AC4: Content-Aware Assessment
- [ ] **Academic**: Higher complexity expected (inflation up to 40% = GOOD)
- [ ] **Blog**: Lower complexity expected (inflation >15% = POOR)
- [ ] **Technical Book**: Moderate complexity (inflation 15-25% = GOOD)
- [ ] **Professional Bio**: Moderate complexity (inflation <20% = EXCELLENT)
- [ ] **Personal Statement**: Natural complexity (inflation <15% = EXCELLENT)

### AC5: Self-Registration
- [ ] Dimension auto-registers via `@register_dimension` decorator
- [ ] Included in `balanced` and `full` dimension profiles
- [ ] Not in `fast` profile (requires frequency corpus lookup)

### AC6: Report Output
- [ ] Shows rare word density and inflation percentage
- [ ] Lists most complex/rare words with frequency ranks
- [ ] Compares to human baseline and content-type expectation
- [ ] Provides actionable suggestions for simplification

### AC7: Performance & Testing
- [ ] Completes analysis in <0.1s for 10,000 word documents
- [ ] Memory <2MB for frequency corpus (or loads on demand)
- [ ] Unit tests achieve ≥85% coverage
- [ ] Validates against research benchmarks (26% inflation threshold)

---

## Tasks/Subtasks

### Task 1: Word Frequency Corpus Integration (0.5 days)
- [ ] Integrate word frequency data (Google Books Ngrams or COCA)
- [ ] Load top 20k words with frequency ranks
- [ ] Implement `get_word_rank(word) -> int` lookup
- [ ] Cache frequency data for performance

### Task 2: Rare Word Detection (0.5 days)
- [ ] Implement `detect_rare_words(text) -> List[Tuple[str, int]]`
  - Returns: `[(word, frequency_rank), ...]`
- [ ] Filter words by rank threshold (>10,000 = rare)
- [ ] Calculate rare word density per 1k words

### Task 3: Complexity Analysis (0.25 days)
- [ ] Implement syllable counting (using pyphen or syllable library)
- [ ] Implement Latinate root detection (suffix patterns: -ize, -ate, -ify, -tion, -ment)
- [ ] Calculate complexity metrics (syllables, length, etymology)

### Task 4: Inflation Score Calculation (0.25 days)
- [ ] Define human baseline: 15 rare words per 1k
- [ ] Calculate inflation percentage vs baseline
- [ ] Content-aware baseline adjustment
- [ ] Score assessment (EXCELLENT/GOOD/NEEDS WORK/POOR)

### Task 5: Dimension Implementation (0.25 days)
- [ ] Create `writescore/dimensions/lexical_inflation.py`
- [ ] Implement `LexicalInflationDimension(BaseDimensionStrategy)`
- [ ] Integrate with dimension registry
- [ ] Add to balanced/full profiles

### Task 6: Testing (0.25 days)
- [ ] Unit tests for rare word detection
- [ ] Unit tests for inflation calculation
- [ ] Content-aware assessment tests
- [ ] Validate 26% inflation threshold detection
- [ ] Performance benchmarks

### Task 7: Documentation (0.1 days)
- [ ] Update CHANGELOG.md
- [ ] Add dimension to README
- [ ] Document research foundation (26% inflation)
- [ ] Provide usage examples

---

## Dev Notes

### Background Context

**Research Foundation** (Liang et al., 2023):
- AI-generated text uses 26% more rare/complex words than human text
- AI favors Latinate vocabulary: utilize (AI), use (human)
- AI inflates syllable counts: 15% more polysyllabic words
- Human baseline: ~15 rare words per 1k words
- AI typical: ~19 rare words per 1k words

**Examples of Lexical Inflation**:
- Human: "I want to **use** tools to **help** people **get** better results"
- AI: "I endeavor to **utilize** methodologies to **facilitate** stakeholders **obtaining** optimal outcomes"

**Content Type Baselines**:

| Content Type | Human Baseline (rare/1k) | Acceptable Inflation | Assessment Threshold |
|--------------|-------------------------|---------------------|---------------------|
| Academic | 25 (higher complexity) | Up to 40% | <35 rare/1k = EXCELLENT |
| Professional Bio | 12 | <20% | <14.4 rare/1k = EXCELLENT |
| Personal Statement | 10 | <15% | <11.5 rare/1k = EXCELLENT |
| Blog | 8 (casual) | <15% | <9.2 rare/1k = EXCELLENT |
| Technical Book | 18 | 15-25% | 20.7-22.5 = GOOD |
| Business | 15 | <25% | <18.75 rare/1k = GOOD |
| Technical Docs | 20 | <20% | <24 rare/1k = GOOD |

### Implementation Strategy

**1. Word Frequency Corpus**
```python
# writescore/dimensions/lexical_inflation.py

# Load frequency data (top 20k words with ranks)
# Source: Google Books Ngrams or COCA
WORD_FREQUENCY_RANKS = {
    'the': 1,
    'be': 2,
    'to': 3,
    # ... common words 1-10,000
    'utilize': 15234,
    'facilitate': 18456,
    'endeavor': 19823,
    # ... rare words 10,001-20,000
}

RARE_WORD_THRESHOLD = 10000  # Words ranked >10k are "rare"

@register_dimension
class LexicalInflationDimension(BaseDimensionStrategy):
    """Detects AI's tendency to use 26% more rare/complex words"""

    dimension_name = "lexical_inflation"

    # Content-specific human baselines (rare words per 1k)
    HUMAN_BASELINES = {
        'academic': 25.0,
        'technical_book': 18.0,
        'technical_docs': 20.0,
        'business': 15.0,
        'professional_bio': 12.0,
        'personal_statement': 10.0,
        'blog': 8.0,
        'creative': 12.0,
        'news': 15.0,
        'generic': 15.0,  # Fallback
    }

    def analyze(self, text: str, content_type: Optional[str] = None) -> Dict[str, Any]:
        """Detect lexical inflation patterns"""
        words = self._tokenize_content_words(text)  # Exclude function words
        word_count = len(words)

        # Detect rare words
        rare_words = self._detect_rare_words(words)
        rare_word_density = (len(rare_words) / word_count) * 1000 if word_count > 0 else 0

        # Calculate inflation vs baseline
        baseline = self.HUMAN_BASELINES.get(content_type or 'generic', 15.0)
        inflation_pct = ((rare_word_density - baseline) / baseline) * 100 if baseline > 0 else 0

        # Complexity metrics
        avg_syllables = self._calculate_avg_syllables(words)
        avg_word_length = sum(len(w) for w in words) / len(words) if words else 0
        latinate_count = self._count_latinate_words(words)
        latinate_density = (latinate_count / word_count) * 1000 if word_count > 0 else 0

        # Assessment
        assessment = self._assess_inflation(inflation_pct, content_type)

        return {
            'rare_word_density': rare_word_density,
            'inflation_pct': inflation_pct,
            'baseline': baseline,
            'avg_syllables': avg_syllables,
            'avg_word_length': avg_word_length,
            'latinate_density': latinate_density,
            'rare_words': rare_words[:10],  # Top 10 for report
            'assessment': assessment,
        }

    def _tokenize_content_words(self, text: str) -> List[str]:
        """Extract content words (exclude function words like the, a, is)"""
        words = re.findall(r'\b[a-z]{3,}\b', text.lower())  # 3+ letter words

        # Exclude common function words
        FUNCTION_WORDS = {'the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'can', 'her', 'was', 'one', 'our', 'out', 'day', 'get', 'has', 'him', 'his', 'how', 'its', 'may', 'new', 'now', 'old', 'see', 'two', 'who', 'boy', 'did', 'its', 'let', 'put', 'say', 'she', 'too', 'use'}

        return [w for w in words if w not in FUNCTION_WORDS]

    def _detect_rare_words(self, words: List[str]) -> List[Tuple[str, int]]:
        """Identify rare words (frequency rank >10,000)"""
        rare_words = []

        for word in words:
            rank = WORD_FREQUENCY_RANKS.get(word, 999999)  # Unknown = very rare

            if rank > RARE_WORD_THRESHOLD:
                rare_words.append((word, rank))

        # Sort by rarity (highest rank = rarest)
        rare_words.sort(key=lambda x: x[1], reverse=True)

        return rare_words

    def _count_latinate_words(self, words: List[str]) -> int:
        """Count words with Latinate suffixes (AI favors these)"""
        LATINATE_SUFFIXES = ['ize', 'ise', 'ate', 'ify', 'tion', 'sion', 'ment', 'ance', 'ence']

        count = 0
        for word in words:
            if any(word.endswith(suffix) for suffix in LATINATE_SUFFIXES):
                count += 1

        return count

    def _calculate_avg_syllables(self, words: List[str]) -> float:
        """Calculate average syllables per word"""
        try:
            from syllables import estimate  # Or use pyphen
            syllable_counts = [estimate(word) for word in words]
            return sum(syllable_counts) / len(syllable_counts) if syllable_counts else 0
        except ImportError:
            # Fallback: vowel group counting
            total_syllables = 0
            for word in words:
                syllables = len(re.findall(r'[aeiouy]+', word))
                total_syllables += max(1, syllables)  # At least 1 syllable

            return total_syllables / len(words) if words else 0

    def _assess_inflation(self, inflation_pct: float, content_type: Optional[str]) -> str:
        """Assess inflation severity"""

        # Content-aware thresholds
        if content_type == 'academic':
            # Higher complexity acceptable
            if inflation_pct <= 40:
                return 'EXCELLENT'
            elif inflation_pct <= 60:
                return 'GOOD'
            else:
                return 'NEEDS WORK'

        elif content_type in ['blog', 'personal_statement']:
            # Lower complexity expected
            if inflation_pct <= 10:
                return 'EXCELLENT'
            elif inflation_pct <= 20:
                return 'GOOD'
            elif inflation_pct <= 30:
                return 'NEEDS WORK'
            else:
                return 'POOR'  # >30% = AI-like for casual content

        else:
            # Generic thresholds (26% research benchmark)
            if inflation_pct <= 10:
                return 'EXCELLENT'
            elif inflation_pct <= 20:
                return 'GOOD'
            elif inflation_pct <= 30:
                return 'NEEDS WORK'
            else:
                return 'POOR'  # >30% = above AI threshold

    def format_results(self, results: Dict[str, Any]) -> str:
        """Format dimension results for report"""
        output = []

        density = results['rare_word_density']
        inflation = results['inflation_pct']
        baseline = results['baseline']

        output.append(f"Rare Word Density: {density:.1f} per 1k words")
        output.append(f"Baseline (human): {baseline:.1f}, Inflation: {inflation:+.1f}%")

        if inflation > 26:
            output.append(f"\n⚠ High lexical inflation detected (AI threshold: +26%)")
            output.append("→ ACTION: Simplify vocabulary, favor common words")

        if results['rare_words']:
            output.append(f"\nRarest Words Detected:")
            for word, rank in results['rare_words'][:5]:
                output.append(f"  • {word} (rank: {rank:,})")

        return "\n".join(output)
```

**2. Example Output**
```
Lexical Inflation: POOR (Inflation: +32%)

Rare Word Density: 19.8 per 1k words
Baseline (human): 15.0, Inflation: +32.0%

⚠ High lexical inflation detected (AI threshold: +26%)
→ ACTION: Simplify vocabulary, favor common words

Rarest Words Detected:
  • endeavor (rank: 19,823)
  • facilitate (rank: 18,456)
  • utilize (rank: 15,234)
  • optimize (rank: 14,567)
  • leverage (rank: 13,789)

Latinate Density: 42.3 per 1k words (AI favors -ize, -ate, -tion suffixes)
Avg Syllables: 2.1 per word (AI typical: 2.0+, Human: 1.7-1.9)
```

### Performance Requirements

- **Analysis Time**: <0.1s for 10,000 word documents
- **Memory**: <2MB for frequency corpus
- **Frequency Lookup**: O(1) via dict

### Testing Requirements

**Unit Tests** (≥85% coverage):
```python
def test_detect_rare_words():
    """Detect words with low frequency"""
    text = "We endeavor to facilitate optimal outcomes by utilizing methodologies."

    dimension = LexicalInflationDimension()
    results = dimension.analyze(text)

    assert results['rare_word_density'] > 15  # Above human baseline
    assert any(word == 'endeavor' for word, _ in results['rare_words'])

def test_inflation_above_ai_threshold():
    """Text with >26% inflation scores POOR"""
    # Artificially inflated text with many rare words
    text = "We endeavor to facilitate optimal synergies utilizing comprehensive methodologies."

    dimension = LexicalInflationDimension()
    results = dimension.analyze(text)

    assert results['inflation_pct'] > 26
    assert results['assessment'] in ['NEEDS WORK', 'POOR']

def test_natural_vocabulary_scores_excellent():
    """Text with common words scores EXCELLENT"""
    text = "I want to use simple tools to help people get better results every day."

    dimension = LexicalInflationDimension()
    results = dimension.analyze(text)

    assert results['inflation_pct'] < 15
    assert results['assessment'] == 'EXCELLENT'
```

### Documentation Requirements

**CHANGELOG.md**:
```markdown
## [1.5.0] - 2025-XX-XX

### Added
- **Lexical Inflation Dimension** (Story 3.6): Detects AI's 26% vocabulary complexity increase
  - Research-proven AI marker (Liang et al., 2023)
  - Measures rare word density vs human baselines
  - Detects Latinate vocabulary preference (utilize vs use)
  - Content-aware baselines (academic=25, blog=8 rare words/1k)
  - Reports inflation percentage and rarest words
```

---

## QA Results

### Test Coverage
- [ ] Unit tests: ___% (Target: ≥85%)
- [ ] Frequency corpus: 20k words loaded
- [ ] Inflation threshold validation: 26% benchmark

### Performance Benchmarks
- [ ] Analysis time (10k words): ___ms (Target: <100ms)
- [ ] Memory usage: ___MB (Target: <2MB)

---

## Change Log

| Date | Author | Change | Status |
|------|--------|--------|--------|
| 2025-01-XX | jmagady | Initial story creation | APPROVED |
