# Story 2.7: Pre-trained Stance Detection Models (Phase 2)

**Status**: Draft
**Estimated Effort**: 2-3 days (16-24 hours)
**Dependencies**:
- Story 2.4.0.5 (Refactor Transition + Pragmatic Dimensions) - ✅ Complete
- Story 2.6 (Expand Pragmatic Markers Lexicon) - ⏳ Recommended prerequisite
**Priority**: MEDIUM
**Target Version**: v6.2.0 or v7.0.0
**Phase**: 2 of 3 (Lexicon Enhancement Strategy)

---

## Executive Summary

**Goal**: Integrate pre-trained transformer models (BERT/RoBERTa) for pragmatic marker detection to achieve 76-80% F1 accuracy.

**Why After Phase 1**:
- **Builds on lexicon baseline**: Phase 1 (75-80% F1) establishes performance target
- **Easier than training from scratch**: No training corpus required, use transfer learning
- **Comparable accuracy to lexicon**: 76-80% F1 (similar to expanded lexicon but ML-based)
- **Prerequisite for Phase 3**: Establishes ML infrastructure before custom BiLSTM-CRF

**Why This Approach**:
- Pre-trained models available on Hugging Face (no training needed)
- Fine-tuned on hedge detection, stance classification tasks
- Easier to implement than BiLSTM-CRF (Story 2.8)
- Provides ML-based comparison to lexicon approach

**Research Foundation**: Perplexity AI Deep Research (November 2025) identified multiple pre-trained BERT/RoBERTa models achieving 76-80% F1 on stance detection and hedge detection tasks, available via Hugging Face Transformers.

---

## Story

**As a** data scientist maintaining WriteScore's pragmatic marker detection,
**I want** to integrate pre-trained transformer models for stance detection,
**so that** we can leverage state-of-the-art NLP models without training infrastructure, achieving 76-80% F1 accuracy with moderate implementation effort.

---

## Background & Research Context

### Current State (Post-Story 2.6)

**File**: `writescore/dimensions/pragmatic_markers.py`
**Approach**: Regex-based pattern matching
**Patterns**: 100-120 total (after Story 2.6 expansion)
**Accuracy**: ~75-80% F1 (lexicon-based)
**Weight**: 4.0%
**Performance**: Fast (<50ms per 1000 words)

### Research Findings (November 2025)

**Source**: Perplexity AI Deep Research - "Hugging Face pre-trained transformer models for hedge detection, stance classification, and uncertainty detection (2024-2025)"

> **IMPORTANT FINDING**: Dedicated pre-trained models specifically for hedge/stance detection are **scarce** on Hugging Face Hub. The recommended approach is **fine-tuning general-purpose models** on benchmark datasets (CoNLL-2010, SemEval-2016).

**Verified Available Models** (Hugging Face - Confirmed to Exist):

1. **roberta-base** (Recommended for Fine-tuning)
   - Model ID: `roberta-base`
   - Architecture: RoBERTa (125M parameters, 12 layers)
   - F1 after fine-tuning on CoNLL-2010: **70-73%** on hedge detection
   - F1 after fine-tuning on SemEval-2016: **65-70%** on stance detection
   - Memory: ~500MB
   - License: Apache 2.0
   - **Best for**: Accuracy-focused deployments

2. **distilbert-base-uncased** (Recommended for Performance)
   - Model ID: `distilbert-base-uncased`
   - Architecture: DistilBERT (66M parameters, 6 layers)
   - F1 after fine-tuning: **66-71%** on hedge detection
   - Memory: ~250MB (40% smaller than BERT)
   - Inference: 60% faster than BERT-base
   - License: Apache 2.0
   - **Best for**: CPU deployments, low-latency requirements

3. **allenai/scibert_scivocab_uncased** (Domain-Specific)
   - Model ID: `allenai/scibert_scivocab_uncased`
   - Architecture: BERT with scientific vocabulary (110M parameters)
   - F1 after fine-tuning: **72-75%** on hedge detection (scientific text)
   - Pre-trained on 1.14M scientific papers
   - License: Apache 2.0
   - **Best for**: Scientific/academic text analysis

4. **dmis-lab/biobert-base-cased-v1.1** (Biomedical Domain)
   - Model ID: `dmis-lab/biobert-base-cased-v1.1`
   - Architecture: BERT pre-trained on PubMed (110M parameters)
   - F1 after fine-tuning: **71-74%** on hedge detection (biomedical text)
   - License: Apache 2.0
   - **Best for**: Biomedical/clinical text

5. **facebook/bart-large-mnli** (Zero-Shot Alternative)
   - Model ID: `facebook/bart-large-mnli`
   - Task: Zero-shot classification (no fine-tuning required)
   - F1 zero-shot: **55-65%** on hedge/stance detection
   - Memory: ~1.6GB (larger)
   - License: MIT
   - **Best for**: Quick prototyping, no labeled data available

**Performance Characteristics**:
- **Inference time**: 50-200ms per 1000 words (10-100× slower than regex)
- **Memory**: 250-500MB for base models, 1.6GB for BART-large
- **Fine-tuning requirement**: All models except BART-MNLI require fine-tuning on CoNLL-2010 or BioScope
- **Advantage**: Context-aware detection (captures implicit hedging)

**Key Finding**: No ready-to-use hedge detection models exist on Hub. Implementation requires either:
1. **Fine-tuning** RoBERTa/DistilBERT on CoNLL-2010 dataset (recommended)
2. **Zero-shot** classification with BART-MNLI (lower accuracy but faster to deploy)

### Comparison to Other Approaches

| Approach | Effort | F1 Accuracy | Inference Time | Memory | Complexity |
|----------|--------|-------------|----------------|--------|------------|
| **Lexicon (52 patterns)** | 0h (baseline) | 68-72% | <10ms | ~1MB | Low |
| **Story 2.6: Expanded Lexicon** | 1-2 days | 75-80% | <50ms | ~1MB | Low |
| **Story 2.7: Pre-trained (This)** | 2-3 days | 76-80% | 50-200ms | 400-800MB | Medium |
| **Story 2.8: BiLSTM-CRF** | 3-5 days | 85.77% | 100-300ms | 200-500MB | High |

**ROI Analysis**:
- **vs Story 2.6**: Similar accuracy (76-80% vs 75-80%), 4× slower, 400× more memory, 2× effort
- **vs Story 2.8**: Lower accuracy (76-80% vs 85.77%), 2× faster, similar memory, 50% less effort
- **Trade-off**: ML-based context awareness vs lexicon simplicity

**When to Use Story 2.7**:
- Need context-aware detection (lexicon misses implicit hedging)
- Want to leverage ML without training infrastructure
- Performance overhead acceptable (50-200ms tolerable)
- Plan to eventually pursue Story 2.8 (establishes ML pipeline)

**When to Skip Story 2.7**:
- Story 2.6 (lexicon) achieves sufficient accuracy (75-80% F1)
- Performance is critical (<50ms required)
- Memory constraints (edge devices, serverless)
- Simplicity preferred over marginal accuracy gain

---

## Acceptance Criteria

### AC1: Model Selection and Evaluation
- [ ] Identify 3-5 candidate pre-trained models from Hugging Face
- [ ] Evaluate each on validation corpus (500+ documents)
- [ ] Measure precision, recall, F1 for hedge detection
- [ ] Measure inference time per 1000 words
- [ ] Select best model (target: F1 ≥ 76%, inference time < 200ms)
- [ ] Document model selection rationale

### AC2: Model Integration Infrastructure
- [ ] Create `core/model_loader.py` for transformer model management
- [ ] Implement lazy loading (load on first use, not import)
- [ ] Implement model caching (avoid reload on every analysis)
- [ ] Add configuration for model selection (BERT vs RoBERTa vs DistilBERT)
- [ ] Handle model download on first run (with user prompt)
- [ ] Error handling for missing models, OOM errors

### AC3: Hybrid Dimension Implementation
- [ ] Create `dimensions/pragmatic_markers_ml.py` (or extend existing)
- [ ] Implement dual-mode scoring:
  - **Lexicon mode**: Fast regex-based (default for <500 words)
  - **ML mode**: Transformer-based (optional for 500+ words)
- [ ] Configuration flag: `use_ml_pragmatic_markers` (default: False)
- [ ] Combine lexicon + ML scores (ensemble approach if both enabled)
- [ ] Maintain backward compatibility (lexicon-only as default)

### AC4: Inference Pipeline
- [ ] Implement sentence-level hedge detection
- [ ] Tokenize text using model's tokenizer
- [ ] Batch inference for efficiency (process multiple sentences)
- [ ] Post-processing: Convert model outputs to hedge counts
- [ ] Aggregate to document-level metrics (per 1k words)
- [ ] Cache results for repeated analyses

### AC5: Performance Optimization
- [ ] GPU support (use CUDA if available, fallback to CPU)
- [ ] Batch size tuning (balance speed vs memory)
- [ ] Quantization option (8-bit for faster inference)
- [ ] Model selection by device (DistilBERT for CPU, RoBERTa for GPU)
- [ ] Inference time < 200ms per 1000 words on CPU
- [ ] Memory usage < 1GB for model + inference

### AC6: Testing and Validation
- [ ] Unit tests for model loader (lazy loading, caching)
- [ ] Unit tests for ML dimension (with mock model)
- [ ] Integration tests with real model (may be slow)
- [ ] Regression tests ensure lexicon mode unchanged
- [ ] Performance benchmarks (time, memory)
- [ ] F1 validation: ≥ 76% on test set

### AC7: Documentation and Configuration
- [ ] CHANGELOG.md updated with v6.2.0/v7.0.0 entry
- [ ] README updated with ML model installation instructions
- [ ] Configuration guide for enabling ML mode
- [ ] Performance characteristics documented (time, memory)
- [ ] Model selection guide (when to use which model)
- [ ] Migration guide for users (opt-in feature)

### AC8: Deployment and Distribution
- [ ] Add transformers library to requirements.txt
- [ ] Add torch/tensorflow to requirements.txt (with version constraints)
- [ ] Create requirements-ml.txt for optional ML dependencies
- [ ] Update installation docs for ML features
- [ ] Test on CPU and GPU environments
- [ ] Test on various platforms (Linux, macOS, Windows)

---

## Tasks / Subtasks

### Phase 0: Model Availability Validation (COMPLETED via Research)

> **Note**: This phase was completed during story authoring via Perplexity Deep Research (November 2025).

- [x] **Task 0: Validate Model Access** *(Pre-requisite for AC1)*
  - [x] Query Perplexity Deep Research: "Hugging Face pre-trained models for hedge detection 2024-2025"
  - [x] **Finding**: No dedicated hedge detection models available on Hub
  - [x] **Finding**: General-purpose models require fine-tuning on CoNLL-2010 or BioScope
  - [x] Verified available models:
    - `roberta-base` (Apache 2.0) - Recommended for accuracy
    - `distilbert-base-uncased` (Apache 2.0) - Recommended for performance
    - `allenai/scibert_scivocab_uncased` (Apache 2.0) - Scientific domain
    - `dmis-lab/biobert-base-cased-v1.1` (Apache 2.0) - Biomedical domain
    - `facebook/bart-large-mnli` (MIT) - Zero-shot alternative
  - [x] Document findings in story Background section
  - [x] **GO Decision**: Proceed with fine-tuning approach using verified models

### Phase A: Model Evaluation and Selection (4-6 hours)

- [ ] **Task 1: Download and Test Candidate Models** (2 hours) *(AC1)*
  - [ ] Download `roberta-base` from Hugging Face
  - [ ] Download `distilbert-base-uncased` from Hugging Face
  - [ ] Download `facebook/bart-large-mnli` for zero-shot comparison
  - [ ] Verify all models load correctly
  - [ ] Measure baseline memory usage for each

- [ ] **Task 2: Preliminary Evaluation** (2-3 hours) *(AC1)*
  - [ ] Create evaluation script: `scripts/evaluate_pretrained_models.py`
  - [ ] Test zero-shot with `facebook/bart-large-mnli` on 100 documents
  - [ ] Test fine-tuned approach simulation with `roberta-base`
  - [ ] Measure inference time per document
  - [ ] Measure memory usage (CPU and GPU)
  - [ ] Rank by: F1 accuracy, speed, memory

- [ ] **Task 3: Select Final Model and Approach** (1-2 hours) *(AC1)*
  - [ ] Decision: Fine-tuning vs Zero-shot approach
  - [ ] If fine-tuning: Select between `roberta-base` (accuracy) or `distilbert-base-uncased` (speed)
  - [ ] If zero-shot: Use `facebook/bart-large-mnli` with appropriate labels
  - [ ] Document decision rationale in `docs/research/MODEL_SELECTION.md`

### Phase B: Infrastructure Implementation (6-8 hours)

- [ ] **Task 4: Create Model Loader** (2-3 hours) *(AC2)*
  - [ ] Implement `core/model_loader.py`:
    ```python
    class PragmaticMarkerModelLoader:
        def __init__(self, model_name: str = "roberta-base"):
            self._model_name = model_name
            self._model = None
            self._tokenizer = None

        def load_model(self) -> Tuple[Any, Any]:
            """Lazy load model and tokenizer."""
            if self._model is None:
                from transformers import AutoModelForSequenceClassification, AutoTokenizer
                self._tokenizer = AutoTokenizer.from_pretrained(self._model_name)
                self._model = AutoModelForSequenceClassification.from_pretrained(self._model_name)
            return self._model, self._tokenizer
    ```
  - [ ] Add lazy loading (load on first use)
  - [ ] Add singleton pattern (one instance per process)
  - [ ] Add error handling (model not found, OOM)

- [ ] **Task 5: Implement Configuration** (1 hour) *(AC2, AC3)*
  - [ ] Add to `core/analysis_config.py`:
    ```python
    @dataclass
    class AnalysisConfig:
        use_ml_pragmatic_markers: bool = False
        ml_model_name: str = "roberta-base"  # Verified model ID
        ml_device: str = "auto"  # auto, cpu, cuda
        ml_batch_size: int = 32
        ml_use_zero_shot: bool = False  # Use BART-MNLI instead of fine-tuned
    ```
  - [ ] Add validation for model name (whitelist verified models)
  - [ ] Add device selection logic (auto-detect GPU)

- [ ] **Task 6: Implement Inference Pipeline** (3-4 hours) *(AC4)*
  - [ ] Create `_ml_analyze_pragmatic_markers()` method
  - [ ] Tokenize text using model tokenizer
  - [ ] Batch sentences for efficiency
  - [ ] Run inference (forward pass)
  - [ ] Post-process outputs to hedge counts
  - [ ] Aggregate to document-level metrics
  - [ ] Handle edge cases (empty text, very long documents)

### Phase C: Dimension Integration (4-6 hours)

- [ ] **Task 7: Extend Pragmatic Markers Dimension** (3-4 hours) *(AC3)*
  - [ ] Add `_ml_mode_enabled()` check to `analyze()`
  - [ ] Implement dual-mode logic:
    ```python
    def analyze(self, text: str) -> Dict[str, Any]:
        if self.config.use_ml_pragmatic_markers and len(text.split()) >= 500:
            return self._ml_analyze(text)
        else:
            return self._lexicon_analyze(text)  # Existing implementation
    ```
  - [ ] Optional: Implement ensemble (combine lexicon + ML)
  - [ ] Add ML-specific metrics to return dict:
    - `ml_mode_used: bool`
    - `ml_confidence: float`
  - [ ] Maintain backward compatibility

- [ ] **Task 8: Scoring Function Updates** (1-2 hours) *(AC3)*
  - [ ] ML scores may have different distribution than lexicon
  - [ ] Calibrate thresholds on validation set (see Dev Notes: Threshold Adjustment)
  - [ ] Maintain 0-100 scoring range
  - [ ] Document any threshold adjustments in `docs/research/ML_CALIBRATION.md`

### Phase D: Testing and Validation (6-8 hours)

- [ ] **Task 9: Unit Tests** (2-3 hours) *(AC6)*
  - [ ] Create `tests/unit/core/test_model_loader.py`
  - [ ] Test model loader (lazy loading, caching)
  - [ ] Test config validation
  - [ ] Test ML mode enabled/disabled
  - [ ] Test fallback to lexicon mode
  - [ ] Mock model for fast tests (no real model loading)
  - [ ] Achieve 85%+ coverage

- [ ] **Task 10: Integration Tests with Real Model** (2-3 hours) *(AC6)*
  - [ ] Create `tests/integration/test_ml_pragmatic_markers.py`
  - [ ] Test full pipeline with real model
  - [ ] Test on sample documents (AI vs human)
  - [ ] Verify F1 ≥ 70% on test set (adjusted target based on research)
  - [ ] Mark with `@pytest.mark.slow` (skip in CI, run manually)

- [ ] **Task 11: Performance Benchmarking** (1-2 hours) *(AC5, AC6)*
  - [ ] Benchmark inference time (CPU and GPU)
  - [ ] Benchmark memory usage
  - [ ] Compare to lexicon baseline
  - [ ] Document performance characteristics
  - [ ] Verify meets acceptance criteria (<200ms per 1k words)

- [ ] **Task 12: Regression Testing** (1 hour) *(AC6)*
  - [ ] Run full test suite with ML disabled
  - [ ] Verify no changes to lexicon mode behavior
  - [ ] Verify backward compatibility maintained
  - [ ] All existing tests pass

### Phase E: Documentation and Deployment (2-4 hours)

- [ ] **Task 13: Update Documentation** (1-2 hours) *(AC7)*
  - [ ] Update CHANGELOG.md with v6.2.0/v7.0.0 entry
  - [ ] Update README with ML model installation:
    ```bash
    # Standard installation (lexicon only)
    pip install writescore

    # ML-enhanced installation (optional)
    pip install writescore[ml]
    ```
  - [ ] Document configuration options
  - [ ] Add performance comparison table (lexicon vs ML)
  - [ ] Add model selection guide

- [ ] **Task 14: Update Dependencies** (0.5 hour) *(AC8)*
  - [ ] Update `setup.py` with extras_require (already includes transformers/torch in base):
    ```python
    extras_require={
        'dev': [...],
        'ml': ['accelerate>=0.20.0'],  # Additional ML optimizations
    }
    ```
  - [ ] Create `requirements-ml.txt` for documentation purposes
  - [ ] Note: transformers>=4.35.0 and torch>=2.0.0 already in base requirements

- [ ] **Task 15: Deployment Testing** (0.5-1 hour) *(AC8)*
  - [ ] Test installation on clean environment
  - [ ] Test ML mode with installed deps
  - [ ] Test on CPU-only machine
  - [ ] Test on GPU machine (if available)
  - [ ] Test model auto-download on first run

---

## Implementation Notes

### Model Selection Criteria

**Evaluation Metrics**:
1. **F1 Score**: ≥ 76% on hedge detection (primary)
2. **Inference Time**: < 200ms per 1000 words (constraint)
3. **Memory**: < 1GB for model + inference (constraint)
4. **Generalizability**: Trained on diverse corpus (not just BioScope medical)
5. **Maintainability**: Active model, recent updates, good documentation

**Verified Available Models** (priority order):

1. **`roberta-base`** (Recommended - Accuracy Focus)
   - Hugging Face ID: `roberta-base`
   - Requires fine-tuning on CoNLL-2010 or BioScope
   - Expected F1 after fine-tuning: 70-73%
   - Memory: ~500MB
   - License: Apache 2.0

2. **`distilbert-base-uncased`** (Recommended - Performance Focus)
   - Hugging Face ID: `distilbert-base-uncased`
   - 40% smaller, 60% faster than BERT
   - Expected F1 after fine-tuning: 66-71%
   - Memory: ~250MB
   - License: Apache 2.0

3. **`facebook/bart-large-mnli`** (Zero-Shot Alternative)
   - Hugging Face ID: `facebook/bart-large-mnli`
   - No fine-tuning required (zero-shot classification)
   - Expected F1: 55-65% (lower but immediate deployment)
   - Memory: ~1.6GB
   - License: MIT

4. **`allenai/scibert_scivocab_uncased`** (Scientific Domain)
   - Best for scientific/academic text
   - Expected F1: 72-75% on scientific corpus
   - License: Apache 2.0

### Inference Pipeline Design

```python
def _ml_analyze_pragmatic_markers(self, text: str) -> Dict[str, Any]:
    """Analyze pragmatic markers using ML model."""

    # Load model (lazy, cached)
    model, tokenizer = self.model_loader.load_model()

    # Sentence segmentation
    sentences = sent_tokenize(text)

    # Batch tokenization
    inputs = tokenizer(
        sentences,
        padding=True,
        truncation=True,
        max_length=128,
        return_tensors="pt"
    )

    # Move to device (CPU or GPU)
    inputs = {k: v.to(self.device) for k, v in inputs.items()}

    # Inference (no gradient computation)
    with torch.no_grad():
        outputs = model(**inputs)
        predictions = torch.argmax(outputs.logits, dim=-1)

    # Post-process: count hedges
    hedge_count = (predictions == HEDGE_LABEL).sum().item()

    # Normalize to per 1k words
    word_count = len(text.split())
    hedge_per_1k = (hedge_count / word_count) * 1000 if word_count > 0 else 0

    return {
        'hedge_count': hedge_count,
        'hedge_per_1k': hedge_per_1k,
        'ml_mode_used': True,
        'ml_confidence': outputs.logits.softmax(dim=-1).max().mean().item(),
        # ... other metrics
    }
```

### Performance Optimization Strategies

1. **Lazy Loading**: Load model only when ML mode first used
2. **Caching**: Keep model in memory across analyses (singleton pattern)
3. **Batching**: Process multiple sentences in single forward pass
4. **Quantization**: Use 8-bit or 16-bit precision (if supported)
5. **Device Selection**: Auto-detect GPU, fallback to CPU
6. **Model Selection**: DistilBERT for CPU, RoBERTa for GPU

### Backward Compatibility

**Default behavior**: ML mode DISABLED (lexicon only)

**Opt-in activation**:
```python
# CLI
python analyze_ai_patterns.py --use-ml-pragmatic-markers input.txt

# Python API
config = AnalysisConfig(use_ml_pragmatic_markers=True)
analyzer = Analyzer(config)
```

**No breaking changes**: Existing users unaffected unless they opt in.

---

## Dev Notes

### Relevant Source Tree

```
writescore/
├── core/
│   ├── analysis_config.py      # MODIFY: Add ML config options (AC2, AC3)
│   │                           # Fields: use_ml_pragmatic_markers, ml_model_name, ml_device
│   └── model_loader.py         # CREATE: Transformer model management (AC2)
│                               # Lazy loading, singleton, error handling
├── dimensions/
│   └── pragmatic_markers.py    # MODIFY: Add dual-mode support (AC3, AC4)
│                               # Current: 126 patterns (post-Story 2.6)
│                               # Add: _ml_analyze() method, mode switching
├── tests/
│   ├── unit/
│   │   └── core/
│   │       └── test_model_loader.py    # CREATE: Model loader unit tests (AC6)
│   └── integration/
│       └── test_ml_pragmatic_markers.py # CREATE: ML integration tests (AC6)
├── scripts/
│   └── evaluate_pretrained_models.py   # CREATE: Model evaluation script (AC1)
├── docs/
│   └── research/
│       ├── MODEL_SELECTION.md          # CREATE: Model selection rationale (AC1)
│       └── ML_CALIBRATION.md           # CREATE: Threshold calibration notes (AC3)
├── requirements.txt            # NO CHANGE: transformers/torch already present
├── setup.py                    # MODIFY: Add 'ml' extras_require (AC8)
└── CHANGELOG.md                # UPDATE: v6.2.0 or v7.0.0 entry (AC7)
```

### From Story 2.6 Implementation

**File Modified**: `writescore/dimensions/pragmatic_markers.py`
**Current Pattern Count**: 126 patterns (post-Story 2.6)
**Current Weight**: 4.0%
**Key Methods to Extend**:
- `analyze()` - Add ML mode check at entry point
- `_score_hedging()` - May need calibration for ML-based scores
- `_calculate_variety_score()` - ML mode returns different metrics

### Testing Standards

**Test Location**:
- Unit tests: `writescore/tests/unit/core/test_model_loader.py` (CREATE)
- Integration tests: `writescore/tests/integration/test_ml_pragmatic_markers.py` (CREATE)

**Framework**: pytest with coverage
**Coverage Target**: 85%+ for new ML code (project minimum: 90%)

**Test Configuration** (from `pytest.ini`):
```bash
# Run unit tests (fast, mocked models)
pytest tests/unit/core/test_model_loader.py -v

# Run integration tests (slow, real models)
pytest tests/integration/test_ml_pragmatic_markers.py -v --timeout=300

# Coverage
pytest --cov=writescore --cov-report=term-missing
```

**Markers**:
- `@pytest.mark.unit` - Fast tests with mocked models
- `@pytest.mark.slow` - Integration tests requiring model download
- `@pytest.mark.integration` - Tests requiring full pipeline

**Mock Strategy**:
```python
# Use unittest.mock to avoid model download in unit tests
@pytest.fixture
def mock_model_loader(mocker):
    mock_model = mocker.MagicMock()
    mock_tokenizer = mocker.MagicMock()
    mocker.patch('writescore.core.model_loader.AutoModelForSequenceClassification.from_pretrained',
                 return_value=mock_model)
    mocker.patch('writescore.core.model_loader.AutoTokenizer.from_pretrained',
                 return_value=mock_tokenizer)
    return mock_model, mock_tokenizer
```

### Security Considerations

**Model Download Security**:
- Models downloaded from Hugging Face Hub on first use via `transformers` library
- Transformers library handles SHA256 checksum verification automatically
- Models cached in `~/.cache/huggingface/` (configurable via `HF_HOME` env var)

**Network Access**:
- First run requires internet access for model download (~250MB-1.6GB)
- Subsequent runs use cached model (no network required)
- Lexicon mode (default) works completely offline

**Graceful Fallback**:
```python
def _ml_analyze(self, text: str) -> Dict[str, Any]:
    try:
        model, tokenizer = self.model_loader.load_model()
        # ... inference
    except Exception as e:
        logger.warning(f"ML mode failed, falling back to lexicon: {e}")
        return self._lexicon_analyze(text)  # Graceful degradation
```

**Recommended Model Whitelist** (verified Apache 2.0 / MIT licensed):
- `roberta-base`
- `distilbert-base-uncased`
- `facebook/bart-large-mnli`
- `allenai/scibert_scivocab_uncased`
- `dmis-lab/biobert-base-cased-v1.1`

### Threshold Adjustment Decision Criteria

**When to Calibrate ML Thresholds** (Task 8):

1. Run both lexicon and ML modes on 500+ document validation corpus
2. Calculate mean and standard deviation for human vs AI documents
3. Adjust ML mode thresholds ONLY IF:
   - Human/AI separation differs by >10% from lexicon mode
   - Score range differs significantly from 0-100 target
4. Document calibration in `docs/research/ML_CALIBRATION.md`

**Expected Behavior**:
- ML mode may detect implicit hedging not captured by lexicon
- This could shift mean hedge counts upward for both human and AI text
- Maintain relative separation (AI/Human ratio ~2:1) even if absolute values change

### Installation Requirements

**Current Dependencies** (already in `setup.py`):
```
transformers>=4.35.0
torch>=2.0.0
```

**Additional ML Extras** (to add to `setup.py`):
```python
extras_require={
    'dev': [...],
    'ml': [
        'accelerate>=0.20.0',  # Hugging Face training optimizations
        'datasets>=2.14.0',    # For loading CoNLL-2010 if fine-tuning
    ],
}
```

**GPU Support** (optional):
```bash
# CUDA 12.1 (if GPU available)
pip install torch --index-url https://download.pytorch.org/whl/cu121
```

---

## Success Metrics

### Quantitative

1. **F1 Accuracy**: ≥ 76% on hedge detection test set
2. **Inference Time**: < 200ms per 1000 words on CPU
3. **Memory Usage**: < 1GB for model + inference
4. **Test Coverage**: 85%+ for new ML code

### Qualitative

5. **Ease of Use**: One-line config to enable ML mode
6. **Documentation**: Clear installation and usage instructions
7. **Backward Compatibility**: No changes for existing users (default disabled)
8. **Model Quality**: Pre-trained model, no training infrastructure required

---

## Related Stories

**Prerequisites**:
- ✅ **Story 2.4.0.5**: Refactor Transition + Pragmatic Dimensions - Complete
- ⏳ **Story 2.6**: Expand Pragmatic Markers Lexicon - Recommended (establishes baseline)

**Leads to**:
- **Story 2.8**: BiLSTM-CRF Advanced Model (Phase 3) - Only if 76-80% F1 insufficient

**Parallel**:
- **Story 2.4.1**: Dimension Scoring Optimization - May benefit from ML-based scores

---

## Decision Point

**Should we implement this story?**

**Yes, if**:
- Story 2.6 (lexicon) completed and F1 < 78% (need improvement)
- Context-aware detection important (lexicon misses implicit hedging)
- Planning to pursue Story 2.8 (establishes ML infrastructure)
- Performance overhead acceptable (50-200ms tolerable)

**No (skip), if**:
- Story 2.6 (lexicon) achieves sufficient accuracy (75-80% F1)
- Performance is critical (<50ms required)
- Memory constraints (edge devices, serverless)
- Simplicity preferred over marginal accuracy gain

**Recommendation**: Implement Story 2.6 first, then re-evaluate. If 75-80% F1 sufficient, skip Story 2.7 and 2.8.

---

## Research Citations

1. **Devlin, J., Chang, M. W., Lee, K., & Toutanova, K.** (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. *NAACL 2019*.
2. **Liu, Y., et al.** (2019). RoBERTa: A Robustly Optimized BERT Pretraining Approach. *arXiv preprint arXiv:1907.11692*.
3. **Sanh, V., Debut, L., Chaumond, J., & Wolf, T.** (2019). DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. *NeurIPS 2019 Workshop*.
4. **Farkas, R., Vincze, V., Móra, G., Csirik, J., & Szarvas, G.** (2010). The CoNLL-2010 Shared Task: Learning to Detect Hedges and Their Scope in Natural Language Text. *Proceedings of CoNLL-2010*.

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-11-23 | 1.0 | Initial story created from Perplexity AI research findings (Phase 2 recommendation) | AI Development Team |
| 2025-11-25 | 1.1 | **PO Validation Remediation**: Updated Research Findings with verified Hugging Face models (original model names did not exist); Added Phase 0 (Model Availability Validation - completed via Perplexity Deep Research); Added Dev Notes section with Source Tree, Testing Standards, Security Considerations, Threshold Adjustment Criteria; Added explicit AC references to all tasks; Updated model recommendations to use verified model IDs (`roberta-base`, `distilbert-base-uncased`, `facebook/bart-large-mnli`); Adjusted F1 targets based on research (70-73% realistic vs 76-80% aspirational) | Sarah (PO) |

---

## Dev Agent Record

> **Note**: This section will be populated by the development agent during implementation.

---

## QA Results

_To be completed by QA agent after implementation_
