# Story 3.1: Content Type Detection

**Status:** ✅ APPROVED
**Parent Epic:** Content-Aware Analysis System (Epic 3.0)
**Estimated Effort:** 2-3 days
**Dependencies:** None
**Priority:** HIGH (Foundation for 3.2, 3.3)
**Target Version:** 1.5.0

---

## Story

As a **technical writer analyzing documents**,
I want **the analyzer to detect or accept content type (academic, blog, technical_book, etc.)**,
so that **dimension weights and scoring thresholds can be adjusted appropriately for the genre**.

---

## Acceptance Criteria

### AC1: CLI Flag Support
- [ ] `--content-type <type>` CLI flag accepts: `academic`, `professional_bio`, `personal_statement`, `blog`, `technical_docs`, `technical_book`, `business`, `creative`, `news`
- [ ] Invalid content types return helpful error with list of valid options
- [ ] Content type is passed through AnalysisConfig to analyzer
- [ ] Analysis report displays specified content type

### AC2: Auto-Detection (Optional)
- [ ] When `--content-type` not provided, analyzer attempts auto-detection
- [ ] Detection returns `(content_type: str, confidence: float)` tuple
- [ ] Confidence ≥0.70 triggers auto-classification with warning message
- [ ] Confidence <0.70 defaults to generic scoring with warning
- [ ] Detection completes in <0.05s for typical documents

### AC3: Feature-Based Classification
- [ ] **First-person pronoun analysis** (I/me/my/we/us/our per 1k words)
  - Academic: <5/1k → `academic`
  - Personal: >20/1k → `personal_statement`
  - Professional bio: 0/1k (3rd person only) → `professional_bio`
- [ ] **Citation markers** (Author-date, numbered refs, bibliography sections)
  - Presence of citations → `academic` (boost confidence +0.15)
- [ ] **Heading patterns** (H1/H2/H3 density and structure)
  - Heavy heading hierarchy → `technical_docs` or `technical_book`
- [ ] **Readability scores** (Flesch-Kincaid Grade Level)
  - FRE <50, Grade 15+ → `academic`
  - FRE 50-70, Grade 9-12 → `technical_book`
  - FRE 60-80, Grade 6-8 → `blog`

### AC4: Multi-Feature Voting Ensemble
- [ ] Implements hard voting: each feature votes for content type
- [ ] Confidence score = agreement ratio (votes for winner / total features)
- [ ] Minimum 4 features vote for classification decision
- [ ] Feature weights: first-person (0.30), citations (0.25), readability (0.20), headings (0.15), vocabulary (0.10)
- [ ] Boundary case detection: if confidence <0.70, log feature vote breakdown

### AC5: AnalysisConfig Integration
- [ ] Add `content_type: Optional[str] = None` to AnalysisConfig dataclass
- [ ] Add `content_type_confidence: Optional[float] = None` to AnalysisConfig
- [ ] Analyzer.analyze() accepts content_type parameter
- [ ] AnalysisResults includes content_type and confidence fields

### AC6: Analysis Report Display
- [ ] Report header shows content type: `Content Type: Technical Book (Confidence: 0.85)`
- [ ] Auto-detected types show warning: `⚠ Auto-detected - specify --content-type to override`
- [ ] Low-confidence detection shows: `⚠ Ambiguous genre (confidence: 0.62) - using generic scoring`

### AC7: Testing Coverage
- [ ] Unit tests achieve ≥85% coverage for content type detection module
- [ ] Test all 9 content types with characteristic examples
- [ ] Test multi-feature voting with edge cases (tied votes, single-feature dominance)
- [ ] Test CLI flag validation and error messages
- [ ] Test auto-detection confidence thresholds

---

## Tasks/Subtasks

### Task 1: Core Detection Module (0.5 days)
- [ ] Create `writescore/core/content_type_detector.py`
- [ ] Implement `ContentTypeDetector` class with feature extractors
- [ ] Implement `detect_content_type(text: str) -> Tuple[str, float]` function
- [ ] Define `CONTENT_TYPES` enum/constants

### Task 2: Feature Extractors (1 day)
- [ ] Implement `extract_first_person_ratio(text) -> float` (I/me/my/we/us/our per 1k)
- [ ] Implement `detect_citation_markers(text) -> bool` (regex for Author, YYYY; [1]; References section)
- [ ] Implement `analyze_heading_structure(text) -> Dict` (H1-H4 counts, density, depth)
- [ ] Implement `calculate_readability_profile(text) -> Dict` (FRE, FK Grade)
- [ ] Implement `analyze_vocabulary_complexity(text) -> float` (rare words, TTR)

### Task 3: Multi-Feature Voting Ensemble (0.5 days)
- [ ] Implement `FeatureVote` dataclass: `(feature_name, predicted_type, confidence)`
- [ ] Implement `aggregate_votes(votes: List[FeatureVote]) -> Tuple[str, float]`
- [ ] Weighted voting with feature importance (first-person: 0.30, citations: 0.25, etc.)
- [ ] Log vote breakdown for low-confidence cases

### Task 4: AnalysisConfig Integration (0.25 days)
- [ ] Add `content_type: Optional[str] = None` to AnalysisConfig
- [ ] Add `content_type_confidence: Optional[float] = None` to AnalysisConfig
- [ ] Update AnalysisResults to include content_type fields
- [ ] Pass content_type through analyzer pipeline

### Task 5: CLI Integration (0.25 days)
- [ ] Add `--content-type` argument to args.py
- [ ] Add validation for valid content types
- [ ] Update main.py to call detector when flag absent
- [ ] Display content type in report header

### Task 6: Testing (0.5 days)
- [ ] Unit tests for each feature extractor
- [ ] Unit tests for voting ensemble
- [ ] Integration tests with sample docs for all 9 types
- [ ] Edge case tests (empty docs, single-sentence docs, mixed genres)
- [ ] CLI flag validation tests

### Task 7: Documentation (0.25 days)
- [ ] Update CHANGELOG.md with Story 3.1 changes
- [ ] Add content type detection section to README
- [ ] Document feature weights and thresholds
- [ ] Add usage examples for --content-type flag

---

## Dev Notes

### Background Context

Different content types have fundamentally different linguistic characteristics. A professional bio *should* have low sentiment variance (factual/neutral), while a personal statement *should* have emotional variation. Current analyzer applies generic scoring that penalizes genre-appropriate characteristics.

**Research Foundation** (60 sources analyzed via Perplexity):
- First-person pronoun frequency is the single most reliable heuristic (70%+ accuracy alone)
- Citation markers provide near-perfect academic identification (95%+ precision)
- Multi-feature voting ensembles achieve 90-94% accuracy for genre classification
- Lightweight feature-based methods outperform deep learning for genre detection when labeled data is limited

**Content Type Characteristics**:

| Content Type | First-Person (per 1k) | Citations | Flesch Grade | Headings |
|--------------|----------------------|-----------|--------------|----------|
| Academic | <5 | High | 15+ | Low |
| Professional Bio | 0 (3rd person) | None | 10-12 | None |
| Personal Statement | 20-50+ | None | 8-10 | Low |
| Blog | 10-30 | Low | 6-8 | Medium |
| Technical Docs | <5 (2nd person) | Low | 9-12 | High |
| Technical Book | 5-15 (mixed) | Medium | 9-12 | High |
| Business | 5-10 (mixed) | None | 10-12 | Medium |
| Creative | 15-40 | None | 6-10 | Low |
| News | <3 (3rd person) | Low | 8-10 | Low |

### Implementation Strategy

**1. Modular Feature Extraction Architecture**
```python
class ContentTypeDetector:
    def __init__(self):
        self.feature_extractors = [
            FirstPersonAnalyzer(),
            CitationDetector(),
            HeadingStructureAnalyzer(),
            ReadabilityProfiler(),
            VocabularyAnalyzer()
        ]

    def detect(self, text: str) -> Tuple[str, float]:
        votes = [extractor.vote(text) for extractor in self.feature_extractors]
        return self.aggregate_votes(votes)
```

**2. Feature-Based Voting**
Each feature extractor returns a vote:
```python
@dataclass
class FeatureVote:
    feature_name: str
    predicted_type: str
    confidence: float
    evidence: Dict[str, Any]  # For debugging/logging
```

**3. Weighted Voting Ensemble**
```python
FEATURE_WEIGHTS = {
    'first_person': 0.30,  # Most reliable single feature
    'citations': 0.25,     # High precision for academic
    'readability': 0.20,   # Distinguishes accessibility levels
    'headings': 0.15,      # Structure indicator
    'vocabulary': 0.10     # Supplementary signal
}

def aggregate_votes(votes: List[FeatureVote]) -> Tuple[str, float]:
    weighted_scores = defaultdict(float)
    for vote in votes:
        weighted_scores[vote.predicted_type] += FEATURE_WEIGHTS[vote.feature_name] * vote.confidence

    winner = max(weighted_scores, key=weighted_scores.get)
    confidence = weighted_scores[winner] / sum(FEATURE_WEIGHTS.values())
    return (winner, confidence)
```

**4. Confidence Thresholds**
- `confidence ≥ 0.70`: Auto-classify with warning
- `confidence < 0.70`: Default to generic scoring, log ambiguity
- `confidence < 0.50`: Likely mixed genre (e.g., technical blog vs technical book)

**5. Boundary Case Handling**
Log feature vote breakdown for manual review:
```
⚠ Ambiguous genre detection (confidence: 0.62)
  Feature votes:
    first_person → personal_statement (0.85)
    readability → blog (0.72)
    headings → technical_book (0.60)
    citations → none detected
  Defaulting to generic scoring. Use --content-type to override.
```

### Relevant Source Tree

```
writescore/
├── core/
│   ├── analysis_config.py        # [MODIFY] Add content_type field
│   ├── analyzer.py                # [MODIFY] Accept content_type param
│   ├── results.py                 # [MODIFY] Add content_type to AnalysisResults
│   └── content_type_detector.py   # [NEW] Detection logic
├── cli/
│   ├── args.py                    # [MODIFY] Add --content-type flag
│   └── main.py                    # [MODIFY] Call detector when flag absent
└── tests/
    └── unit/
        └── core/
            └── test_content_type_detector.py  # [NEW] 85%+ coverage
```

### Technical Implementation Details

#### First-Person Pronoun Analysis
```python
class FirstPersonAnalyzer:
    FIRST_PERSON_PRONOUNS = ['i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours']

    def vote(self, text: str) -> FeatureVote:
        words = text.lower().split()
        word_count = len(words)

        first_person_count = sum(1 for word in words if word in self.FIRST_PERSON_PRONOUNS)
        ratio = (first_person_count / word_count) * 1000  # per 1k words

        if ratio < 3:
            # Could be academic, professional_bio, or news
            # Check for 3rd person indicators
            if self._is_third_person_bio(text):
                return FeatureVote('first_person', 'professional_bio', 0.85, {'ratio': ratio})
            return FeatureVote('first_person', 'academic', 0.70, {'ratio': ratio})
        elif ratio > 20:
            return FeatureVote('first_person', 'personal_statement', 0.90, {'ratio': ratio})
        elif 10 <= ratio <= 20:
            return FeatureVote('first_person', 'blog', 0.75, {'ratio': ratio})
        else:
            return FeatureVote('first_person', 'technical_book', 0.60, {'ratio': ratio})
```

#### Citation Detection
```python
class CitationDetector:
    CITATION_PATTERNS = [
        r'\([A-Z][a-z]+,?\s+\d{4}\)',      # (Author, 2023) or (Author 2023)
        r'\[[0-9]+\]',                      # [1], [2]
        r'\b(References|Bibliography|Works Cited)\b',  # Section headers
        r'\bet al\.',                       # et al.
    ]

    def vote(self, text: str) -> FeatureVote:
        citation_count = 0
        for pattern in self.CITATION_PATTERNS:
            citation_count += len(re.findall(pattern, text))

        if citation_count >= 3:
            return FeatureVote('citations', 'academic', 0.95, {'count': citation_count})
        elif citation_count == 1-2:
            return FeatureVote('citations', 'technical_book', 0.60, {'count': citation_count})
        else:
            return FeatureVote('citations', 'blog', 0.50, {'count': 0})
```

#### Readability Profile
```python
class ReadabilityProfiler:
    def vote(self, text: str) -> FeatureVote:
        flesch_ease = textstat.flesch_reading_ease(text)
        fk_grade = textstat.flesch_kincaid_grade(text)

        if flesch_ease < 50 and fk_grade >= 15:
            return FeatureVote('readability', 'academic', 0.85, {'fre': flesch_ease, 'grade': fk_grade})
        elif 50 <= flesch_ease <= 70 and 9 <= fk_grade <= 12:
            return FeatureVote('readability', 'technical_book', 0.80, {'fre': flesch_ease, 'grade': fk_grade})
        elif flesch_ease > 70 and fk_grade < 9:
            return FeatureVote('readability', 'blog', 0.85, {'fre': flesch_ease, 'grade': fk_grade})
        else:
            return FeatureVote('readability', 'business', 0.60, {'fre': flesch_ease, 'grade': fk_grade})
```

#### Heading Structure Analysis
```python
class HeadingStructureAnalyzer:
    def vote(self, text: str) -> FeatureVote:
        headings = re.findall(r'^#{1,4}\s+.+$', text, re.MULTILINE)
        heading_density = len(headings) / max(1, len(text.split('\n')))

        h1_count = len([h for h in headings if h.startswith('# ')])
        h2_count = len([h for h in headings if h.startswith('## ')])
        h3_count = len([h for h in headings if h.startswith('### ')])

        if heading_density > 0.15 and h3_count > 5:
            # Heavy hierarchy suggests technical documentation
            if h2_count > 20:
                return FeatureVote('headings', 'technical_book', 0.80, {'density': heading_density, 'h2': h2_count})
            return FeatureVote('headings', 'technical_docs', 0.75, {'density': heading_density})
        elif heading_density > 0.05:
            return FeatureVote('headings', 'blog', 0.70, {'density': heading_density})
        else:
            return FeatureVote('headings', 'creative', 0.60, {'density': heading_density})
```

### Performance Requirements

- **Detection Time**: <0.05s for documents up to 10,000 words
- **Memory**: <500KB additional memory usage
- **Accuracy**: ≥85% agreement with manual classification on test corpus
- **Benchmark**: Process 100 documents in <5 seconds total

### Testing Requirements

**Unit Tests** (≥85% coverage):
```python
def test_first_person_analyzer_academic():
    text = "The study examines the effects of climate change. Results indicate significant correlation."
    analyzer = FirstPersonAnalyzer()
    vote = analyzer.vote(text)
    assert vote.predicted_type == 'academic'
    assert vote.confidence >= 0.70

def test_citation_detector_academic():
    text = "Recent research (Smith, 2023) demonstrates that machine learning models perform better with diverse training data."
    detector = CitationDetector()
    vote = detector.vote(text)
    assert vote.predicted_type == 'academic'
    assert vote.confidence >= 0.90

def test_multi_feature_voting_technical_book():
    text = """
    # Chapter 1: Introduction to Machine Learning

    I've spent the last decade working with ML systems. Let me show you what I've learned.

    ## What is Machine Learning?

    Machine learning is a subset of artificial intelligence...
    """
    detector = ContentTypeDetector()
    content_type, confidence = detector.detect(text)
    assert content_type == 'technical_book'
    assert confidence >= 0.70
```

**Integration Tests**:
- Test all 9 content types with characteristic sample documents
- Test CLI flag: `pytest tests/integration/test_cli_content_type.py`
- Test auto-detection: `pytest tests/integration/test_auto_detection.py`

**Edge Cases**:
- Empty document → default to 'business', confidence 0.0
- Single sentence → low confidence (<0.50), use generic scoring
- Mixed genre (academic blog post) → log ambiguity, highest-voted type

### Documentation Requirements

**CHANGELOG.md**:
```markdown
## [1.5.0] - 2025-XX-XX

### Added
- **Content Type Detection** (Story 3.1): Analyzer now detects or accepts content type via `--content-type` flag
  - Supports 9 content types: academic, professional_bio, personal_statement, blog, technical_docs, technical_book, business, creative, news
  - Auto-detection using multi-feature voting ensemble (first-person pronouns, citations, readability, headings, vocabulary)
  - Confidence scoring with ≥0.70 threshold for auto-classification
  - Foundation for content-aware dimension weighting (Story 3.2) and scoring thresholds (Story 3.3)
```

**README.md**:
```markdown
## Content Type Detection

The analyzer can detect or accept content type for appropriate dimension weighting:

```bash
# Specify content type explicitly
python analyze_ai_patterns.py chapter-01.md --content-type technical_book

# Auto-detect (shows confidence)
python analyze_ai_patterns.py essay.md
# Output: Content Type: Personal Statement (Confidence: 0.85, Auto-detected)

# Supported types: academic, professional_bio, personal_statement, blog,
#                  technical_docs, technical_book, business, creative, news
```

**Feature Weights**: First-person pronouns (30%), citations (25%), readability (20%), headings (15%), vocabulary (10%)
```

---

## QA Results

### Test Coverage
- [ ] Unit tests: ___% (Target: ≥85%)
- [ ] Integration tests: ___ passing
- [ ] Edge case tests: ___ passing

### Performance Benchmarks
- [ ] Detection time: ___ms (Target: <50ms)
- [ ] Memory usage: ___KB (Target: <500KB)
- [ ] Accuracy on test corpus: ___% (Target: ≥85%)

### Manual Validation
- [ ] Tested all 9 content types with characteristic examples
- [ ] Verified CLI flag validation and error messages
- [ ] Confirmed auto-detection warnings display correctly
- [ ] Checked low-confidence boundary case logging

---

## Change Log

| Date | Author | Change | Status |
|------|--------|--------|--------|
| 2025-01-XX | jmagady | Initial story creation | APPROVED |
