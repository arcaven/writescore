# Story 2.8: Advanced Pragmatic Markers Dimension (BiLSTM-CRF) - Phase 3

**Status**: Draft
**Parent Epic**: docs/stories/epic-2-enhanced-ai-detection.md
**Estimated Effort**: 3-5 days (new dimension with ML model integration)
**Dependencies**:
- Story 2.4.0.5 (Refactor Transition + Pragmatic Dimensions) - ✅ Complete
- Story 2.6 (Expand Pragmatic Markers Lexicon) - ⏳ Recommended prerequisite
- Story 2.7 (Pre-trained Stance Detection) - ⏳ Optional prerequisite
**Priority**: LOW (Phase 3 - Only if 75-80% F1 insufficient)
**Target Version**: v7.0.0 or later
**Phase**: 3 of 3 (Lexicon Enhancement Strategy)

---

## Research Update (November 2025)

**Strategic Recommendation**: This story represents **Phase 3** (advanced ML) and should only be pursued if:
1. **Phase 1** (Story 2.6: Lexicon expansion to 100-120 patterns, 75-80% F1) proves insufficient
2. **Phase 2** (Story 2.7: Pre-trained models, 76-80% F1) proves insufficient
3. The 5-10% absolute F1 improvement (from 75-80% to 85.77%) justifies the additional effort and complexity

**Effort vs Accuracy Analysis**:
- **Story 2.6 (Lexicon)**: 1-2 days → 75-80% F1
- **Story 2.7 (Pre-trained)**: 2-3 days → 76-80% F1
- **Story 2.8 (This - BiLSTM-CRF)**: 3-5 days → 85.77% F1

**ROI Decision Point**: Implement Stories 2.6 and 2.7 first. Re-evaluate if 85.77% F1 truly needed for use cases.

**Source**: Perplexity AI Deep Research (November 2025) - "Epistemic stance markers and pragmatic communication patterns in NLP (2023-2025)"

---

## Story

**As a** content analyst requiring high-accuracy pragmatic marker detection,
**I want** an optional ML-based dimension using BiLSTM-CRF for advanced hedging and scope detection,
**so that** I can achieve 85%+ F1 accuracy with scope identification when processing time is not critical.

---

## Business Value

### Problem Statement

The current TransitionMarkerDimension v1.2.0 (Story 2.2.1) provides:
- **Coverage**: 52 patterns (~50% of Hyland's taxonomy)
- **Accuracy**: F1 ~68-72% (estimated)
- **Performance**: 0.035s per 10k words
- **Use case**: Fast, discriminative AI detection with good accuracy

However, some use cases require:
- **Comprehensive coverage**: 85%+ of hedging/pragmatic patterns
- **Higher accuracy**: F1 85%+ for academic writing analysis
- **Scope detection**: Identify exact span of hedged content
- **Context sensitivity**: Distinguish epistemic hedging from other uses (e.g., "may" as permission vs. uncertainty)

### Value Proposition

This story adds an **optional** BiLSTM-CRF-based dimension that:
1. **Increases accuracy from ~68-72% to ~85.77% F1** (+13-17% improvement, state-of-the-art)
2. **Detects hedge scope**: Identifies exact span of hedged content (unique capability)
3. **Context-aware**: Distinguishes epistemic hedging from other modal uses (e.g., "may" as permission vs. uncertainty)
4. **No training required**: Uses pre-trained CoNLL-2010 models
5. **Opt-in architecture**: Default analyzer remains fast; users enable when needed

### Success Metrics

- F1 score ≥85% on CoNLL-2010 benchmark
- Hedge scope detection accuracy ≥80%
- Processing time ≤1.5s per 10k words (50% better than BERT)
- Memory footprint ≤350MB
- Zero impact on default analyzer performance

---

## Acceptance Criteria

### AC1: BiLSTM-CRF Model Integration
- [ ] Pre-trained BiLSTM-CRF model loaded from CoNLL-2010 shared task
- [ ] Model supports both hedge detection and scope identification
- [ ] Model loaded lazily (only when dimension is enabled)
- [ ] Fallback mechanism if model loading fails

### AC2: Advanced Pragmatic Markers Dimension
- [ ] New dimension class: `AdvancedPragmaticMarkersDimension`
- [ ] Inherits from `BaseAnalyzer`
- [ ] Implements `analyze()` method with BiLSTM-CRF inference
- [ ] Returns results compatible with existing dimension output format
- [ ] Version: v1.0.0

### AC3: Hedge Scope Detection
- [ ] Scope detection implemented using CRF layer output
- [ ] Results include:
  - Hedge marker position (start, end)
  - Scope boundary (start, end)
  - Confidence score (0.0-1.0)
  - Hedged content text
- [ ] Scope accuracy ≥80% on BioScope corpus evaluation set

### AC4: Results Schema
- [ ] Results dictionary includes:
  - `hedge_count`: Total hedges detected
  - `hedge_per_1k`: Hedges per 1k words
  - `avg_confidence`: Average detection confidence
  - `scopes`: List of hedge scope objects
  - `coverage_pct`: Percentage of text covered by hedging
  - `context_types`: Classification of hedge types (epistemic, deontic, dynamic)
- [ ] Compatible with existing dimension result structure

### AC5: Optional Dimension Architecture
- [ ] CLI flag: `--enable-advanced-pragmatics` to activate dimension
- [ ] Config option: `enable_advanced_pragmatics: true` in YAML config
- [ ] Dimension NOT loaded by default (maintains fast performance)
- [ ] Clear documentation on when to use vs. TransitionMarkerDimension v1.2.0

### AC6: Performance Requirements
- [ ] Processing time ≤1.5s per 10k words (50% better than BERT)
- [ ] Memory footprint ≤350MB (competitive with similar models)
- [ ] Model loading time ≤3s on first use
- [ ] Batch processing optimization for multiple documents

### AC7: Dependencies and Installation
- [ ] Dependencies: `torch`, `numpy`, `transformers` (optional group)
- [ ] Installation: `pip install writescore[advanced]`
- [ ] Clear error message if dependencies missing
- [ ] Documentation for installing optional dependencies

### AC8: Test Coverage
- [ ] 85%+ unit test coverage for new dimension
- [ ] 15+ test cases:
  - Model loading and initialization
  - Hedge detection accuracy tests
  - Scope detection accuracy tests
  - Edge cases (empty text, no hedges, all hedges)
  - Performance benchmarks
  - Fallback behavior
- [ ] Integration test with full analyzer
- [ ] Regression test using CoNLL-2010 test set sample

### AC9: Documentation
- [ ] CHANGELOG.md updated with v5.2.0 entry
- [ ] Class docstrings with usage examples
- [ ] README section: "Advanced Pragmatic Markers (Optional)"
- [ ] Performance comparison table (regex vs. BiLSTM-CRF)
- [ ] Use case guide: when to use each approach

---

## Tasks / Subtasks

- [ ] **Task 1: Research and Model Selection** (AC: 1) (4-6 hours)
  - [ ] Survey available pre-trained BiLSTM-CRF models for hedging
  - [ ] Evaluate CoNLL-2010 shared task winning models
  - [ ] Test model accuracy on sample texts
  - [ ] Verify license compatibility (MIT/Apache 2.0 preferred)
  - [ ] Document model provenance and citation requirements

- [ ] **Task 2: Model Integration** (AC: 1, 2) (6-8 hours)
  - [ ] Create `dimensions/advanced_pragmatic.py` module
  - [ ] Implement `AdvancedPragmaticMarkersDimension` class
  - [ ] Load pre-trained BiLSTM-CRF model weights
  - [ ] Implement lazy loading mechanism
  - [ ] Add model caching to avoid reloading
  - [ ] Implement fallback if model unavailable

- [ ] **Task 3: Hedge Detection Implementation** (AC: 2, 3) (6-8 hours)
  - [ ] Implement `_detect_hedges(text: str) -> List[HedgeInstance]`
  - [ ] Use BiLSTM-CRF for token-level classification
  - [ ] Extract hedge markers with confidence scores
  - [ ] Implement confidence threshold filtering (default: 0.5)
  - [ ] Handle multi-word hedge expressions

- [ ] **Task 4: Scope Detection Implementation** (AC: 3) (8-10 hours)
  - [ ] Implement `_detect_scope(text: str, hedge_positions: List) -> List[ScopeInstance]`
  - [ ] Use CRF layer to identify scope boundaries
  - [ ] Extract hedged content text
  - [ ] Calculate scope metrics (avg scope length, coverage)
  - [ ] Validate scope accuracy on BioScope corpus sample

- [ ] **Task 5: Results Schema** (AC: 4) (2-3 hours)
  - [ ] Define `HedgeInstance` dataclass (marker, position, confidence)
  - [ ] Define `ScopeInstance` dataclass (hedge, scope_start, scope_end, content)
  - [ ] Implement `analyze()` method returning complete results dict
  - [ ] Add `_calculate_metrics()` for summary statistics
  - [ ] Ensure compatibility with existing dimension output format

- [ ] **Task 6: Scoring Logic** (AC: 4) (3-4 hours)
  - [ ] Implement `_calculate_score() -> float` (0-100 scale)
  - [ ] Score based on:
    - Hedge density (per 1k words)
    - Scope coverage (% of text hedged)
    - Hedge variety (unique hedge types)
    - Context appropriateness
  - [ ] Align scoring with TransitionMarkerDimension v1.1.0
  - [ ] Research-backed thresholds for human vs. AI ranges

- [ ] **Task 7: Optional Dimension Architecture** (AC: 5) (4-5 hours)
  - [ ] Update `core/dimension_registry.py` to support optional dimensions
  - [ ] Add `enabled: false` flag to dimension metadata
  - [ ] Implement CLI flag: `--enable-advanced-pragmatics`
  - [ ] Add config YAML support: `enable_advanced_pragmatics: true`
  - [ ] Update analyzer to skip disabled dimensions
  - [ ] Document opt-in architecture

- [ ] **Task 8: Performance Optimization** (AC: 6) (6-8 hours)
  - [ ] Benchmark baseline performance (before optimization)
  - [ ] Implement batch processing for multiple sentences
  - [ ] Optimize tokenization and preprocessing
  - [ ] Add GPU support if CUDA available
  - [ ] Implement model quantization (FP16) if performance insufficient
  - [ ] Validate final performance: ≤1.5s per 10k words

- [ ] **Task 9: Dependency Management** (AC: 7) (2-3 hours)
  - [ ] Update `setup.py` with `[advanced]` extras group
  - [ ] Add torch, numpy, transformers to optional dependencies
  - [ ] Implement graceful import handling:
    ```python
    try:
        import torch
    except ImportError:
        raise ImportError("Install advanced dependencies: pip install writescore[advanced]")
    ```
  - [ ] Update installation documentation
  - [ ] Test installation in clean virtual environment

- [ ] **Task 10: Unit Tests** (AC: 8) (8-10 hours)
  - [ ] Create `tests/unit/dimensions/test_advanced_pragmatic.py`
  - [ ] Test hedge detection accuracy (precision, recall, F1)
  - [ ] Test scope detection accuracy
  - [ ] Test edge cases:
    - Empty text
    - No hedges detected
    - All text is hedged
    - Multi-word hedge expressions
  - [ ] Test confidence threshold filtering
  - [ ] Test performance benchmarks
  - [ ] Test fallback behavior (model unavailable)
  - [ ] Test lazy loading mechanism
  - [ ] Achieve 85%+ code coverage

- [ ] **Task 11: Integration Tests** (AC: 8) (4-5 hours)
  - [ ] Create integration test: dimension registration
  - [ ] Test CLI flag `--enable-advanced-pragmatics`
  - [ ] Test YAML config integration
  - [ ] Test output format compatibility
  - [ ] Test batch processing mode
  - [ ] Verify no impact on default analyzer performance

- [ ] **Task 12: Validation and Benchmarking** (AC: 8) (4-6 hours)
  - [ ] Create `validate_performance_story_2_6.py` script
  - [ ] Download CoNLL-2010 test set sample (200 sentences)
  - [ ] Run accuracy benchmark: F1 ≥85%
  - [ ] Run scope accuracy benchmark: ≥80%
  - [ ] Run performance benchmark: ≤1.5s per 10k words
  - [ ] Run memory benchmark: ≤350MB
  - [ ] Document benchmark results

- [ ] **Task 13: Documentation** (AC: 9) (4-5 hours)
  - [ ] Update CHANGELOG.md with v5.2.0 entry
  - [ ] Write comprehensive class docstrings with examples
  - [ ] Add README section: "Advanced Pragmatic Markers (Optional)"
  - [ ] Create comparison table: TransitionMarkerDimension vs. AdvancedPragmaticMarkersDimension
  - [ ] Document use cases:
    - Fast AI detection: Use TransitionMarkerDimension
    - Academic analysis: Use AdvancedPragmaticMarkersDimension
  - [ ] Add installation guide for optional dependencies
  - [ ] Document model citation and license

---

## Technical Approach

### Model Architecture

```
BiLSTM-CRF Architecture:
┌─────────────────────────────────────────┐
│ Input: Tokenized text                   │
└─────────────────────────────────────────┘
                 │
                 ▼
┌─────────────────────────────────────────┐
│ Word Embeddings (GloVe/Word2Vec)        │
│ Dimension: 100-300                      │
└─────────────────────────────────────────┘
                 │
                 ▼
┌─────────────────────────────────────────┐
│ Bidirectional LSTM                      │
│ Hidden size: 200-400                    │
│ Layers: 2                               │
└─────────────────────────────────────────┘
                 │
                 ▼
┌─────────────────────────────────────────┐
│ Linear layer → Tag scores               │
│ Tags: O, B-hedge, I-hedge, B-scope,     │
│       I-scope                           │
└─────────────────────────────────────────┘
                 │
                 ▼
┌─────────────────────────────────────────┐
│ CRF Layer (Viterbi decoding)            │
│ Outputs: Best tag sequence              │
└─────────────────────────────────────────┘
                 │
                 ▼
┌─────────────────────────────────────────┐
│ Output: Hedges + Scopes                 │
└─────────────────────────────────────────┘
```

### Implementation Outline

```python
# dimensions/advanced_pragmatic.py

from typing import Dict, List, Any, Optional
from dataclasses import dataclass
import torch
import numpy as np
from dimensions.base_strategy import BaseAnalyzer

@dataclass
class HedgeInstance:
    """Detected hedge instance."""
    text: str
    start_pos: int
    end_pos: int
    confidence: float
    hedge_type: str  # epistemic, deontic, dynamic

@dataclass
class ScopeInstance:
    """Hedge scope instance."""
    hedge: HedgeInstance
    scope_start: int
    scope_end: int
    scope_text: str
    confidence: float

class AdvancedPragmaticMarkersDimension(BaseAnalyzer):
    """
    BiLSTM-CRF-based pragmatic marker detection with scope identification.

    **Performance:**
    - F1 Score: ~85.77% (CoNLL-2010 benchmark)
    - Processing: ~1.0-1.5s per 10k words
    - Memory: ~300-350MB

    **Use Cases:**
    - Academic writing analysis requiring high accuracy
    - Hedge scope identification for detailed feedback
    - Context-sensitive hedge classification

    **When to use TransitionMarkerDimension v1.2.0 instead:**
    - Fast AI detection (0.035s vs 1.5s) - still 43x faster
    - Production pipelines with strict performance requirements
    - Lightweight deployments (zero dependencies)
    - Good accuracy (F1 ~68-72%) sufficient for most use cases

    **Version History:**
    - v1.0.0 (2025-11): Initial release with BiLSTM-CRF integration

    **Citation:**
    Based on CoNLL-2010 Shared Task winning models:
    - Morante, R., & Daelemans, W. (2009). Learning the scope of hedge cues
      in biomedical texts. CoNLL-2009.

    **Example:**
    ```python
    dimension = AdvancedPragmaticMarkersDimension()
    config = AnalysisConfig(mode=AnalysisMode.FULL)
    results = dimension.analyze(text, config)

    print(f"Hedges: {results['hedge_count']}")
    print(f"F1 Score equivalent: ~85.77%")
    for scope in results['scopes']:
        print(f"Hedge: {scope.hedge.text}")
        print(f"Scope: {scope.scope_text}")
    ```
    """

    def __init__(self):
        super().__init__(
            name="advanced_pragmatic",
            weight=10.0,
            tier="ADVANCED"
        )
        self.model = None  # Lazy loading
        self.tokenizer = None

    def _load_model(self):
        """Lazy load BiLSTM-CRF model."""
        if self.model is not None:
            return

        try:
            import torch
            from transformers import AutoTokenizer
            # Load pre-trained CoNLL-2010 BiLSTM-CRF model
            # TODO: Specify exact model path/URL after research
            self.model = self._load_bilstm_crf_model()
            self.tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
        except ImportError as e:
            raise ImportError(
                "Advanced pragmatic markers require optional dependencies. "
                "Install with: pip install writescore[advanced]"
            ) from e

    def analyze(self, text: str, config: Any = None) -> Dict[str, Any]:
        """
        Analyze text for pragmatic markers using BiLSTM-CRF.

        Args:
            text: Input text to analyze
            config: Analysis configuration

        Returns:
            Dict containing:
            - hedge_count: Total hedges detected
            - hedge_per_1k: Hedges per 1000 words
            - avg_confidence: Average detection confidence
            - scopes: List of ScopeInstance objects
            - coverage_pct: Percentage of text hedged
            - context_types: Hedge type distribution
            - score: 0-100 score (inverse of AI-likeness)
        """
        self._load_model()

        # Detect hedges
        hedges = self._detect_hedges(text)

        # Detect scopes
        scopes = self._detect_scopes(text, hedges)

        # Calculate metrics
        metrics = self._calculate_metrics(text, hedges, scopes)

        # Calculate score
        score = self._calculate_score(metrics)

        return {
            'hedge_count': len(hedges),
            'hedge_per_1k': metrics['hedge_per_1k'],
            'avg_confidence': metrics['avg_confidence'],
            'scopes': scopes,
            'coverage_pct': metrics['coverage_pct'],
            'context_types': metrics['context_types'],
            'score': score,
            'hedges': hedges  # Full hedge details
        }

    def _detect_hedges(self, text: str) -> List[HedgeInstance]:
        """Detect hedge markers using BiLSTM-CRF."""
        # TODO: Implement BiLSTM-CRF inference
        pass

    def _detect_scopes(self, text: str, hedges: List[HedgeInstance]) -> List[ScopeInstance]:
        """Detect hedge scopes using CRF layer."""
        # TODO: Implement scope detection
        pass

    def _calculate_metrics(self, text: str, hedges: List[HedgeInstance],
                          scopes: List[ScopeInstance]) -> Dict[str, Any]:
        """Calculate summary metrics."""
        word_count = len(text.split())
        total_hedges = len(hedges)

        # Calculate hedges per 1k words
        hedge_per_1k = (total_hedges / word_count * 1000) if word_count > 0 else 0

        # Calculate average confidence
        avg_confidence = np.mean([h.confidence for h in hedges]) if hedges else 0

        # Calculate coverage (% of text in hedge scopes)
        total_scope_chars = sum(s.scope_end - s.scope_start for s in scopes)
        coverage_pct = (total_scope_chars / len(text) * 100) if len(text) > 0 else 0

        # Classify hedge types
        context_types = {
            'epistemic': sum(1 for h in hedges if h.hedge_type == 'epistemic'),
            'deontic': sum(1 for h in hedges if h.hedge_type == 'deontic'),
            'dynamic': sum(1 for h in hedges if h.hedge_type == 'dynamic'),
        }

        return {
            'hedge_per_1k': hedge_per_1k,
            'avg_confidence': avg_confidence,
            'coverage_pct': coverage_pct,
            'context_types': context_types
        }

    def _calculate_score(self, metrics: Dict[str, Any]) -> float:
        """
        Calculate 0-100 score (higher = more human-like).

        Thresholds based on research:
        - Human: 4-7 hedges per 1k words
        - AI: 10-15 hedges per 1k words
        """
        hedge_freq = metrics['hedge_per_1k']
        coverage = metrics['coverage_pct']

        # Score hedging frequency
        if 4.0 <= hedge_freq <= 7.0:
            freq_score = 100.0  # Human range
        elif hedge_freq < 4.0:
            freq_score = 80.0 - (4.0 - hedge_freq) * 10  # Too few hedges
        elif hedge_freq <= 9.0:
            freq_score = 80.0 - (hedge_freq - 7.0) * 10  # Moderate overuse
        else:
            freq_score = max(0.0, 60.0 - (hedge_freq - 9.0) * 5)  # AI overuse

        # Score coverage (high coverage = AI-like)
        if coverage <= 15.0:
            coverage_score = 100.0
        elif coverage <= 25.0:
            coverage_score = 80.0 - (coverage - 15.0) * 2
        else:
            coverage_score = max(0.0, 60.0 - (coverage - 25.0) * 2)

        # Weighted composite
        score = freq_score * 0.7 + coverage_score * 0.3
        return round(score, 2)
```

### Results Schema

```python
{
    'hedge_count': 42,
    'hedge_per_1k': 8.4,
    'avg_confidence': 0.87,
    'coverage_pct': 18.3,
    'context_types': {
        'epistemic': 35,
        'deontic': 4,
        'dynamic': 3
    },
    'score': 72.5,
    'scopes': [
        {
            'hedge': {
                'text': 'may',
                'start_pos': 145,
                'end_pos': 148,
                'confidence': 0.91,
                'hedge_type': 'epistemic'
            },
            'scope_start': 145,
            'scope_end': 189,
            'scope_text': 'may indicate a broader trend in the data',
            'confidence': 0.88
        },
        # ... more scopes
    ],
    'hedges': [
        # Full hedge details
    ]
}
```

---

## Performance Targets

| Metric | Target | Rationale |
|--------|--------|-----------|
| **F1 Score** | ≥85% | Match CoNLL-2010 SOTA |
| **Scope Accuracy** | ≥80% | BioScope corpus benchmark |
| **Processing Time** | ≤1.5s per 10k words | 50% better than BERT |
| **Memory** | ≤350MB | Competitive with BiLSTM models |
| **Model Load Time** | ≤3s | Acceptable for first use |
| **Test Coverage** | ≥85% | Standard quality requirement |

---

## Trade-offs and Alternatives

### Why BiLSTM-CRF over BERT?

| Criterion | BiLSTM-CRF | BERT | Decision |
|-----------|------------|------|----------|
| **Speed** | 1.0-1.5s | 2.0s | BiLSTM-CRF wins (2x faster) |
| **Accuracy** | 85.77% F1 | 85.77% F1 | Tie |
| **Memory** | 300-350MB | 500MB | BiLSTM-CRF wins (40% less) |
| **Scope Detection** | Native CRF support | Requires additional layer | BiLSTM-CRF wins |
| **Interpretability** | Partial transparency | Black box | BiLSTM-CRF wins |

**Conclusion**: BiLSTM-CRF provides same accuracy as BERT with better performance and scope detection capabilities.

### Why Optional Dimension?

- **Default use case**: Fast AI detection (TransitionMarkerDimension v1.2.0 sufficient for most cases)
- **Performance impact**: 43x slower than regex (1.5s vs 0.035s)
- **Accuracy gap**: +13-17% F1 improvement (v1.2.0: ~68-72%, BiLSTM-CRF: ~85.77%)
- **Dependencies**: Adds torch, numpy, transformers (large footprint ~500MB+)
- **User control**: Let users choose speed vs. accuracy trade-off
- **Unique capabilities**: Scope detection and context-aware classification only available in BiLSTM-CRF

### Comparison: TransitionMarkerDimension v1.2.0 vs. AdvancedPragmaticMarkersDimension

| Criterion | TransitionMarkerDimension v1.2.0 | AdvancedPragmaticMarkersDimension | When to Use |
|-----------|----------------------------------|-----------------------------------|-------------|
| **Patterns** | 52 (regex) | 100+ (learned) | v1.2.0 for known patterns |
| **Coverage** | ~50% of Hyland's taxonomy | ~95% of hedging patterns | BiLSTM for comprehensive |
| **F1 Accuracy** | ~68-72% (estimated) | ~85.77% (CoNLL-2010) | BiLSTM for high accuracy |
| **Speed (10k words)** | 0.035s | 1.5s | v1.2.0 for production (43x faster) |
| **Memory** | 0.15KB | 350MB | v1.2.0 for lightweight (2.3M x less) |
| **Dependencies** | None (stdlib regex) | torch, transformers, numpy | v1.2.0 for simplicity |
| **Scope Detection** | No | Yes (identifies hedged spans) | BiLSTM for detailed analysis |
| **Context-Aware** | No | Yes (distinguishes modal uses) | BiLSTM for disambiguation |
| **Implementation** | Story 2.2.1 (2-3 hours) | Story 2.6 (3-5 days) | v1.2.0 for quick wins |
| **False Positives** | Medium (~25-30%) | Low (~10%) | BiLSTM for precision |

**Value Proposition After Story 2.2.1:**

Story 2.2.1's expanded lexicon (31 → 52 patterns) significantly improves the baseline, making BiLSTM-CRF's value more focused:

- **Accuracy improvement**: +13-17% F1 (moderate) vs. +27% F1 before Story 2.2.1 (major)
- **Primary value**: Shifts from "much better accuracy" to "unique capabilities" (scope detection, context awareness)
- **Use cases**: More specialized (academic analysis, hedge scope needed) vs. general AI detection

**Recommendation**:

Use TransitionMarkerDimension v1.2.0 for **90% of use cases**. Only use BiLSTM-CRF when:
- F1 accuracy >70% is critical (academic research, linguistic analysis)
- Hedge scope identification needed (detailed feedback, writing analysis)
- Context-aware classification required (distinguish "may" as permission vs. uncertainty)
- Processing time secondary to accuracy (batch processing, offline analysis)
- False positive rate must be minimized (<10% vs. 25-30%)

**ROI Analysis:**

| Approach | Effort | F1 Improvement | Effort-to-Benefit Ratio |
|----------|--------|----------------|-------------------------|
| Story 2.2.1 (v1.2.0) | 2-3 hours | +10-14% | Excellent (5% per hour) |
| Story 2.6 (BiLSTM) | 3-5 days | +13-17% (over v1.2.0) | Moderate (0.4% per hour) |

Story 2.2.1 provides better ROI for accuracy improvement. Story 2.6's value lies in **unique capabilities**, not just accuracy gains.

---

## Research References

1. **CoNLL-2010 Shared Task**: Learning to Detect Hedges and Their Scope in Natural Language Text
   - Winning F1: 85.77%
   - Dataset: BioScope corpus + Wikipedia uncertainty corpus
   - URL: https://www.aclweb.org/anthology/W10-3001/

2. **Morante & Daelemans (2009)**: Learning the Scope of Hedge Cues in Biomedical Texts
   - BiLSTM-CRF architecture for hedge detection
   - Scope identification using CRF layer

3. **SIGDIAL 2024**: Training LLMs to Recognize Hedges in Spontaneous Dialogues
   - Recent benchmarks showing BiLSTM-CRF competitive with BERT

4. **BioScope Corpus**: 20,924 sentences with hedge and scope annotations
   - Standard benchmark for hedge detection
   - Available: https://rgai.inf.u-szeged.hu/node/105

5. **Hyland (1996, 1998)**: Hedging in Scientific Research Articles
   - Foundational taxonomy of hedging devices

---

## Risk Assessment

| Risk | Severity | Mitigation |
|------|----------|------------|
| **Pre-trained model unavailable** | Medium | Provide fallback to fine-tuning script; consider hosting model weights |
| **Performance worse than target** | Medium | Implement model quantization (FP16), GPU support, batch optimization |
| **License incompatibility** | Low | Verify license during research; prefer MIT/Apache models |
| **Integration complexity** | Medium | Comprehensive testing; clear documentation; optional architecture reduces risk |
| **Dependency bloat** | Low | Optional install group keeps default analyzer lightweight |

---

## Success Criteria

**Story is considered done when:**

1. ✅ All 9 acceptance criteria validated
2. ✅ F1 score ≥85% on CoNLL-2010 test sample
3. ✅ Scope accuracy ≥80% on BioScope corpus
4. ✅ Processing time ≤1.5s per 10k words
5. ✅ 85%+ test coverage, all tests passing
6. ✅ Documentation complete (CHANGELOG, README, docstrings)
7. ✅ Optional dimension architecture working (CLI + config)
8. ✅ Zero impact on default analyzer performance
9. ✅ QA gate PASS decision

---

## QA Notes

### Review Checklist

- [ ] All acceptance criteria implemented and tested
- [ ] Benchmark results documented (F1, scope accuracy, performance)
- [ ] Pre-trained model license verified
- [ ] Optional dimension architecture tested
- [ ] No performance regression in default analyzer
- [ ] Installation guide tested in clean environment
- [ ] Comparison documentation clear (when to use vs. TransitionMarkerDimension)

### Performance Validation

```bash
# Run performance validation
python validate_performance_story_2_6.py

# Expected output:
# ✓ F1 Score ≥85%: 85.8% - PASS
# ✓ Scope Accuracy ≥80%: 81.2% - PASS
# ✓ Processing Time ≤1.5s: 1.32s - PASS
# ✓ Memory ≤350MB: 312 MB - PASS
# ✓ ALL PERFORMANCE CHECKS PASSED
```

---

## Related Stories

- **Story 2.2**: Pragmatic Markers Dimension (Enhanced) - Foundation (31 patterns, v1.1.0)
- **Story 2.2.1**: Expand Pragmatic Marker Lexicon - Prerequisite (52 patterns, v1.2.0)
  - **Impact**: Reduces accuracy gap from +27% F1 to +13-17% F1
  - **Recommendation**: Implement Story 2.2.1 first (better ROI: 2-3 hours vs. 3-5 days)
- **Story 2.5**: Percentile-Anchored Scoring Parameters - Complementary
- **Future Story 2.7**: Hybrid Pragmatic Detection (Regex + ML) - Alternative approach

---

## Open Questions

1. **Model Selection**: Which CoNLL-2010 winning model to use? (Top 3 had similar F1 scores)
   - Evaluate: Model size, inference speed, license, availability

2. **GPU Support**: Should we optimize for GPU acceleration?
   - Pro: 5-10x speedup on GPU
   - Con: Adds CUDA dependency complexity

3. **Scope Visualization**: Should we add scope visualization in reports?
   - Pro: Helps users understand hedge scope
   - Con: Increases complexity of output formatting

4. **Fine-tuning**: Should we provide fine-tuning script for domain adaptation?
   - Pro: Users can adapt to specific domains
   - Con: Requires training data and expertise

---

**Status**: Draft - Ready for review and approval
