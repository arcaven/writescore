# Story 2.9: Short Content Optimization (Adaptive Scoring for <500 Words)

**Epic**: 2.0 - Advanced AI Detection Dimensions
**Status**: PROPOSED
**Priority**: HIGH
**Estimated Effort**: 3-4 days
**Dependencies**: None (enhances existing dimensions)
**Target Version**: v6.3.0 or v7.0.0

---

## Problem Statement

The current algorithm fails to provide meaningful scores for short-form content (<500 words) due to:

1. **Advanced Lexical Dimension Returns N/A**: HDD, Yule's K, and MATTR require minimum 500 words for statistical reliability, returning `None` for shorter text
2. **Fixed Weight Distribution**: Advanced Lexical's 14% weight is lost when metrics are N/A, artificially lowering overall quality scores
3. **MATTR Segment Size Issues**: Fixed 100-token segment size fails for documents with <100 tokens
4. **Poor User Experience**: Users analyzing application statements, emails, social media posts, or short articles receive incomplete analysis with "N/A" metrics
5. **Misleading Quality Gaps**: Reports suggest "+10.5 pts" from fixing N/A dimensions, but this is impossible for short content

**Real-World Impact**:
- Application statement (317 words) achieved 80.9/100 quality score but reports "+4.1 pts gap" primarily due to N/A Advanced Lexical metrics
- Short-form content (emails, tweets, paragraphs) cannot be comprehensively analyzed
- Users perceive the tool as broken or unreliable for common use cases

---

## Proposed Solution

Implement **adaptive scoring** with three-tiered analysis based on document length:

### Tier 1: MICRO Analysis (<100 words)
**Active Dimensions**: 6 core dimensions (60% coverage)
- Perplexity, Burstiness, Voice, Lexical (basic TTR), Sentiment, Formatting

**Disabled**:
- Advanced Lexical (requires 500+ words)
- Structure (requires multiple sections)
- Readability (unreliable on <100 words)
- Syntactic (needs varied sentence patterns)
- Predictability (GLTR requires context)
- Transition Markers (needs paragraph structure)
- Figurative Language (needs sufficient context)

### Tier 2: SHORT Analysis (100-499 words)
**Active Dimensions**: 10 dimensions (76% coverage)
- All MICRO dimensions
- \+ Structure, Syntactic, Readability, Transition Markers

**Adaptive Behavior**:
- **Advanced Lexical**: Use TTR-based approximation (see below)
- **MATTR**: Adaptive segment sizing (10-100 tokens based on text length)
- **Predictability**: Reduced weight or approximation

### Tier 3: STANDARD Analysis (500+ words)
**Active Dimensions**: All 13 dimensions (100% coverage)
- Full analysis with all advanced metrics

---

## Technical Implementation

### 1. Advanced Lexical Fallback for Short Text

**File**: `dimensions/advanced_lexical.py`

**Current Code** (lines 201-208):
```python
# Handle None values (text too short for calculation)
if hdd is None:
    hdd = 0.5  # Neutral default
if yules_k is None:
    yules_k = 100.0  # Neutral default
```

**Proposed Solution**:
```python
def _calculate_advanced_lexical_diversity(self, text: str) -> Dict[str, Any]:
    """Calculate advanced lexical metrics with fallback for short text."""
    doc = nlp_spacy(text)
    tokens = [t.text.lower() for t in doc if not t.is_punct and not t.is_space]
    types = set(tokens)

    token_count = len(tokens)
    type_count = len(types)

    # Short text mode: Use TTR-based approximations
    if token_count < 500:
        ttr = type_count / token_count if token_count > 0 else 0.0

        return {
            'hdd_score': min(1.0, ttr * 0.85 + 0.15),  # TTR-based approximation (0.15-1.0 range)
            'yules_k': max(0, 150 - (ttr * 150)),      # Inverse TTR (0-150 range)
            'mattr': ttr * 0.9,                        # Simplified MATTR
            'rttr': type_count / math.sqrt(token_count) if token_count > 0 else 0.0,
            'maas': (math.log(token_count) - math.log(type_count)) / math.log(token_count)**2 if token_count > 1 and type_count > 1 else 0.0,
            'types': type_count,
            'tokens': token_count,
            'short_text_mode': True,
            'approximation_method': 'TTR-based',
            'reliability': 'ESTIMATED'  # Flag for user awareness
        }

    # Standard calculation for 500+ words
    return self._calculate_full_lexical_diversity(doc, tokens, types)
```

**Rationale**:
- Provides scores instead of N/A
- Based on proven TTR correlation with HDD/Yule's K
- Flagged as 'ESTIMATED' to set user expectations
- Allows short content to be scored and compared

### 2. Adaptive MATTR Segment Sizing

**File**: `dimensions/advanced_lexical.py` (lines 418-426)

**Current Code**:
```python
try:
    mattr = diversity.segmented_ttr(doc, segment_size=100, variant='moving-avg')
except Exception as e:
    print(f"Warning: MATTR calculation failed, trying smaller segment: {e}")
    try:
        mattr = diversity.segmented_ttr(doc, segment_size=50, variant='moving-avg')
    except Exception:
        mattr = None
```

**Proposed Solution**:
```python
# Adaptive segment sizing based on document length
token_count = len([t for t in doc if not t.is_punct])
segment_size = self._calculate_optimal_segment_size(token_count)

try:
    if segment_size >= 10:  # Minimum viable segment
        mattr = diversity.segmented_ttr(doc, segment_size=segment_size, variant='moving-avg')
    else:
        # Ultra-short text: use TTR as MATTR approximation
        mattr = self._calculate_simple_ttr(doc)
except Exception as e:
    # Graceful degradation
    mattr = self._calculate_simple_ttr(doc)

def _calculate_optimal_segment_size(self, token_count: int) -> int:
    """
    Calculate optimal MATTR segment size based on document length.

    Research shows segment size should be 1/3 to 1/2 of total tokens
    for meaningful moving average, with 10-100 token bounds.

    Args:
        token_count: Total tokens in document

    Returns:
        Segment size (10-100)
    """
    if token_count < 30:
        return 0  # Too short for MATTR
    elif token_count < 100:
        return max(10, token_count // 3)
    elif token_count < 300:
        return 50
    else:
        return 100  # Standard segment size
```

### 3. Dynamic Weight Distribution

**File**: `core/weight_mediator.py`

**Current Behavior**: Fixed 14% weight for Advanced Lexical even when N/A

**Proposed Solution**:
```python
class WeightMediator:
    """Manages dynamic dimension weights based on content characteristics."""

    BASE_WEIGHTS = {
        'perplexity': 8.0,
        'burstiness': 12.0,
        'structure': 10.0,
        'voice': 8.0,
        'formatting': 6.0,
        'syntactic': 9.0,
        'sentiment': 7.0,
        'readability': 8.0,
        'lexical': 10.0,
        'predictability': 8.0,
        'advanced_lexical': 14.0,
        'transition_marker': 10.0,
        'figurative_language': 10.0,
    }

    def calculate_adaptive_weights(
        self,
        word_count: int,
        active_dimensions: List[str],
        config: AnalysisConfig
    ) -> Dict[str, float]:
        """
        Calculate adaptive weights based on document characteristics.

        Args:
            word_count: Total words in document
            active_dimensions: List of dimension names to include
            config: Analysis configuration

        Returns:
            Dict mapping dimension names to weights (summing to 100.0)
        """
        weights = self.BASE_WEIGHTS.copy()

        # Tier 1: MICRO (<100 words)
        if word_count < 100:
            # Disable heavy dimensions
            disabled = [
                'advanced_lexical', 'structure', 'readability',
                'syntactic', 'predictability', 'transition_marker',
                'figurative_language'
            ]
            available_weight = sum(weights[d] for d in disabled)
            for dim in disabled:
                weights[dim] = 0.0

            # Redistribute to active core dimensions
            core_dims = ['perplexity', 'burstiness', 'voice', 'lexical', 'sentiment', 'formatting']
            per_dim = available_weight / len(core_dims)
            for dim in core_dims:
                weights[dim] += per_dim

        # Tier 2: SHORT (100-499 words)
        elif word_count < 500:
            # Advanced Lexical uses approximation (reduced confidence)
            # Redistribute 50% of its weight
            advanced_weight = weights['advanced_lexical']
            weights['advanced_lexical'] = advanced_weight * 0.5

            # Give to lexical and burstiness (more reliable on short text)
            weights['lexical'] += advanced_weight * 0.3
            weights['burstiness'] += advanced_weight * 0.2

            # Predictability may be approximated (reduce weight)
            if 'predictability' in weights:
                pred_weight = weights['predictability']
                weights['predictability'] = pred_weight * 0.7
                weights['syntactic'] += pred_weight * 0.3

        # Tier 3: STANDARD (500+ words) - use base weights

        # Filter to only active dimensions
        active_weights = {k: v for k, v in weights.items() if k in active_dimensions}

        # Normalize to 100%
        total = sum(active_weights.values())
        if total > 0:
            return {k: (v / total) * 100.0 for k, v in active_weights.items()}
        else:
            return {}
```

### 4. User-Facing Messaging

**File**: `cli/formatters.py`

**Add Analysis Tier Information**:
```python
def format_analysis_report(results: AnalysisResults, mode: Optional[str] = None) -> str:
    """Format analysis report with tier information for short content."""

    word_count = results.total_words
    tier = _determine_analysis_tier(word_count)

    # Add tier notification for SHORT and MICRO tiers
    tier_notice = ""
    if tier == "MICRO":
        tier_notice = f"""
{'!' * 80}
MICRO ANALYSIS MODE (Document: {word_count} words)
- Using 6 core dimensions (60% coverage)
- Advanced metrics unavailable for <100 word documents
- For comprehensive analysis, expand to 500+ words
{'!' * 80}
"""
    elif tier == "SHORT":
        tier_notice = f"""
{'─' * 80}
SHORT CONTENT MODE (Document: {word_count} words)
- Using 10 dimensions with adaptive scoring (76% coverage)
- Advanced Lexical metrics: ESTIMATED (TTR-based approximation)
- For full analysis with research-grade metrics, expand to 500+ words
{'─' * 80}
"""

    # Include in report header
    report = f"""
{'=' * 80}
AI PATTERN ANALYSIS REPORT{f' (Mode: {mode.upper()})' if mode else ''}
{'=' * 80}

File: {results.file_path}
Words: {results.total_words} | Sentences: {results.total_sentences} | Paragraphs: {results.total_paragraphs}
Analysis Tier: {tier}
{tier_notice}
"""
    # ... rest of report
```

**Add Reliability Flags to Metrics**:
```python
# In dimension score output
if r.advanced_lexical_score and word_count < 500:
    report += f"""
Advanced Lexical (Richness): {fmt_score(r.advanced_lexical_score)}  (HDD: {fmt(r.hdd_score, '.2f')}*, Yule's K: {fmt(r.yules_k, '.1f')}*)
  * ESTIMATED - TTR-based approximation for short text (<500 words)"""
else:
    report += f"""
Advanced Lexical (Richness): {fmt_score(r.advanced_lexical_score)}  (HDD: {fmt(r.hdd_score, '.2f')}, Yule's K: {fmt(r.yules_k, '.1f')})"""
```

---

## Acceptance Criteria

### Core Functionality

1. **Tier Detection**:
   - [ ] System correctly identifies MICRO (<100w), SHORT (100-499w), and STANDARD (500+w) tiers
   - [ ] Appropriate dimensions are activated/deactivated per tier
   - [ ] Tier information is displayed in analysis report header

2. **Advanced Lexical Fallback**:
   - [ ] HDD, Yule's K, MATTR return valid scores (not N/A) for <500 word documents
   - [ ] Scores are flagged as "ESTIMATED" with reliability indicator
   - [ ] TTR-based approximation correlates within ±15% of full calculation when tested on truncated 500+ word docs

3. **Adaptive MATTR**:
   - [ ] Segment size adapts to document length: 10-100 token range
   - [ ] Documents with 30-100 tokens receive MATTR scores (not N/A)
   - [ ] Documents with <30 tokens gracefully fall back to simple TTR

4. **Dynamic Weighting**:
   - [ ] Dimension weights redistribute when metrics are unavailable
   - [ ] Total weights sum to 100% for all tier configurations
   - [ ] Quality scores for short content no longer penalized for N/A dimensions

### User Experience

5. **Clear Messaging**:
   - [ ] MICRO tier displays warning about limited coverage (60%)
   - [ ] SHORT tier displays info about estimated metrics
   - [ ] STANDARD tier displays no special notices (business as usual)
   - [ ] Recommendations guide users to expand content when appropriate

6. **Quality Score Accuracy**:
   - [ ] 317-word application statement scores 85%+ (vs current 80.9% with N/A penalty)
   - [ ] 100-word paragraph receives meaningful score (not dominated by N/A gaps)
   - [ ] 50-word snippet analyzed with appropriate 6-dimension subset

### Testing & Validation

7. **Regression Tests**:
   - [ ] Existing 500+ word documents maintain same scores (±1 point tolerance)
   - [ ] All existing unit tests pass without modification
   - [ ] Performance remains <2s for short documents (<500 words)

8. **New Test Coverage**:
   - [ ] Unit tests for each tier (MICRO, SHORT, STANDARD)
   - [ ] Unit tests for TTR-based approximation accuracy
   - [ ] Unit tests for adaptive segment sizing
   - [ ] Integration tests for weight redistribution

9. **Documentation**:
   - [ ] User guide updated with tier system explanation
   - [ ] API documentation includes `analysis_tier`, `short_text_mode`, `reliability` fields
   - [ ] Migration guide for users expecting old behavior

---

## Implementation Tasks

### Phase 1: Core Adaptive Scoring (1.5 days)

1. **Create Tier Detection Logic** (3 hours)
   - [ ] Implement `_determine_analysis_tier(word_count)` function
   - [ ] Add tier detection to `Analyzer.analyze()` entry point
   - [ ] Create `AnalysisTier` enum (MICRO, SHORT, STANDARD)

2. **Implement Advanced Lexical Fallback** (4 hours)
   - [ ] Add `_calculate_short_text_lexical()` method in `advanced_lexical.py`
   - [ ] Implement TTR-based HDD/Yule's K approximation
   - [ ] Add `short_text_mode` and `reliability` flags to return dict
   - [ ] Test approximation accuracy on truncated documents

3. **Adaptive MATTR Segment Sizing** (2 hours)
   - [ ] Implement `_calculate_optimal_segment_size()` method
   - [ ] Update MATTR calculation to use adaptive sizing
   - [ ] Add graceful degradation for ultra-short text (<30 tokens)

4. **Unit Tests for Fallback Logic** (2.5 hours)
   - [ ] Test TTR-based approximation vs full calculation
   - [ ] Test MATTR segment sizing for various lengths (30w, 100w, 250w, 500w)
   - [ ] Test edge cases (empty text, single sentence, single word)

### Phase 2: Dynamic Weighting System (1 day)

5. **Implement WeightMediator** (4 hours)
   - [ ] Create `core/weight_mediator.py` with `WeightMediator` class
   - [ ] Implement `calculate_adaptive_weights()` method
   - [ ] Add weight redistribution logic for each tier
   - [ ] Integrate with `DualScoreCalculator`

6. **Update Dimension Registry** (2 hours)
   - [ ] Add tier compatibility metadata to each dimension
   - [ ] Implement dimension filtering based on tier
   - [ ] Update `DimensionLoader` to respect tier constraints

7. **Integration & Testing** (2 hours)
   - [ ] Test weight normalization (sum = 100%)
   - [ ] Verify scores improve for short content
   - [ ] Test that 500+ word docs maintain scores

### Phase 3: User Experience & Messaging (0.5 days)

8. **Update CLI Output** (2 hours)
   - [ ] Add tier notice to report header (`cli/formatters.py`)
   - [ ] Add reliability flags to estimated metrics (HDD*, Yule's K*)
   - [ ] Update recommendations to mention word count thresholds

9. **Update JSON/TSV Output** (1 hour)
   - [ ] Add `analysis_tier` field to JSON output
   - [ ] Add `short_text_mode` and `reliability` fields per dimension
   - [ ] Update TSV schema documentation

10. **Documentation Updates** (1 hour)
    - [ ] Update README with tier system explanation
    - [ ] Add examples for MICRO, SHORT, STANDARD analysis
    - [ ] Document when to use each tier

### Phase 4: Comprehensive Testing (1 day)

11. **Create Test Corpus** (2 hours)
    - [ ] Collect 10 documents per tier (30 total)
    - [ ] Include: tweets (50w), emails (150w), paragraphs (250w), essays (500w+)
    - [ ] Baseline scores with current system

12. **Regression Testing** (2 hours)
    - [ ] Run full test suite on 500+ word corpus
    - [ ] Verify scores within ±1 point tolerance
    - [ ] Check for performance regressions

13. **Validation Testing** (2 hours)
    - [ ] Analyze application statement (317w) - target 85%+ score
    - [ ] Analyze short email (150w) - verify meaningful scoring
    - [ ] Analyze tweet (50w) - verify 6-dimension subset works

14. **Performance Testing** (2 hours)
    - [ ] Benchmark analysis time for each tier
    - [ ] Ensure MICRO/SHORT faster than STANDARD (less computation)
    - [ ] Profile weight calculation overhead

---

## Testing Strategy

### Unit Tests

**File**: `tests/unit/core/test_weight_mediator.py`
```python
def test_micro_tier_weight_distribution():
    """Test weight redistribution for <100 word documents."""
    mediator = WeightMediator()
    weights = mediator.calculate_adaptive_weights(
        word_count=75,
        active_dimensions=['perplexity', 'burstiness', 'voice', 'lexical', 'sentiment', 'formatting'],
        config=DEFAULT_CONFIG
    )

    # Verify sum = 100%
    assert abs(sum(weights.values()) - 100.0) < 0.01

    # Verify disabled dimensions have 0 weight
    assert 'advanced_lexical' not in weights
    assert 'structure' not in weights

    # Verify core dimensions have increased weights
    assert weights['burstiness'] > 12.0  # Should be higher than base
    assert weights['lexical'] > 10.0

def test_short_tier_advanced_lexical_reduction():
    """Test Advanced Lexical weight reduction for 100-499 word documents."""
    mediator = WeightMediator()
    weights = mediator.calculate_adaptive_weights(
        word_count=317,
        active_dimensions=list(WeightMediator.BASE_WEIGHTS.keys()),
        config=DEFAULT_CONFIG
    )

    # Advanced Lexical should be reduced to ~7% (50% of 14%)
    assert 6.0 < weights['advanced_lexical'] < 8.0

    # Lexical should receive boost
    assert weights['lexical'] > 10.0
```

**File**: `tests/unit/dimensions/test_advanced_lexical.py`
```python
def test_short_text_fallback_approximation():
    """Test TTR-based approximation for <500 word documents."""
    dim = AdvancedLexicalDimension()

    # 317-word application statement
    text = open('/tmp/application_statement_v4.txt').read()
    result = dim.analyze(text)

    # Should return valid scores, not None
    assert result['hdd_score'] is not None
    assert result['yules_k'] is not None
    assert result['mattr'] is not None

    # Should be flagged as short text mode
    assert result['short_text_mode'] is True
    assert result['reliability'] == 'ESTIMATED'

    # Scores should be in reasonable ranges
    assert 0.0 <= result['hdd_score'] <= 1.0
    assert 0.0 <= result['yules_k'] <= 200.0

def test_approximation_accuracy():
    """Verify TTR-based approximation is within ±15% of full calculation."""
    dim = AdvancedLexicalDimension()

    # Load 1000-word document
    full_text = load_test_document('1000_word_essay.txt')

    # Calculate full metrics
    full_result = dim.analyze(full_text)
    full_hdd = full_result['hdd_score']
    full_yules = full_result['yules_k']

    # Truncate to 400 words and use approximation
    short_text = ' '.join(full_text.split()[:400])
    short_result = dim.analyze(short_text)
    approx_hdd = short_result['hdd_score']
    approx_yules = short_result['yules_k']

    # Verify correlation within tolerance
    # Note: We expect approximation to be close but not exact
    hdd_error = abs(approx_hdd - full_hdd) / full_hdd if full_hdd > 0 else 0
    yules_error = abs(approx_yules - full_yules) / full_yules if full_yules > 0 else 0

    assert hdd_error < 0.20  # Within 20% (relaxed for truncated text)
    assert yules_error < 0.20
```

### Integration Tests

**File**: `tests/integration/test_short_content_analysis.py`
```python
def test_application_statement_complete_scoring():
    """Test that 317-word application statement receives complete scoring."""
    analyzer = Analyzer()

    text = open('/tmp/application_statement_v4.txt').read()
    results = analyzer.analyze(text, mode=AnalysisMode.FULL)

    # Should have valid quality score
    assert results.dual_score is not None
    assert results.dual_score.quality_score >= 85.0  # Should hit target now

    # Advanced Lexical should have valid scores (not N/A)
    assert results.hdd_score is not None
    assert results.yules_k is not None

    # Should be SHORT tier
    assert results.analysis_tier == 'SHORT'

def test_micro_tier_six_dimensions():
    """Test that <100 word documents use only 6 core dimensions."""
    analyzer = Analyzer()

    # 75-word paragraph
    text = "This is a short paragraph with limited content. " * 10
    results = analyzer.analyze(text[:500], mode=AnalysisMode.FULL)

    # Should be MICRO tier
    assert results.analysis_tier == 'MICRO'

    # Should have 6 active dimensions
    active_dims = [d for d in results.dimension_scores if d.score is not None]
    assert len(active_dims) == 6

    # Should still have meaningful quality score
    assert 0 <= results.dual_score.quality_score <= 100
```

### Performance Tests

**File**: `tests/performance/test_short_content_performance.py`
```python
def test_micro_tier_faster_than_standard():
    """Verify MICRO tier analysis is faster than STANDARD (fewer dimensions)."""
    analyzer = Analyzer()

    # 75-word text (MICRO)
    micro_text = generate_text(75)
    start = time.time()
    analyzer.analyze(micro_text, mode=AnalysisMode.FULL)
    micro_time = time.time() - start

    # 1000-word text (STANDARD)
    standard_text = generate_text(1000)
    start = time.time()
    analyzer.analyze(standard_text, mode=AnalysisMode.FULL)
    standard_time = time.time() - start

    # MICRO should be at least 2x faster (fewer dimensions + less text)
    assert micro_time < standard_time / 2.0
```

---

## Success Metrics

### Quantitative

1. **Score Improvement for Short Content**:
   - 317-word application statement: 80.9% → 85%+ quality score
   - 150-word email: N/A scores → valid scores in all active dimensions
   - 75-word paragraph: Complete analysis with 6 dimensions

2. **N/A Reduction**:
   - Advanced Lexical N/A rate: 100% (<500w) → 0% (all documents)
   - Overall N/A metrics: Reduce by 90% for short content

3. **Performance**:
   - MICRO tier analysis: <0.5s
   - SHORT tier analysis: <1.5s
   - STANDARD tier analysis: Unchanged (<2.5s)

### Qualitative

4. **User Experience**:
   - Clear messaging about tier and limitations
   - Actionable recommendations (expand to 500+ words)
   - No false "gaps to target" for unavoidable limitations

5. **Accuracy**:
   - TTR-based approximation within ±15% of full calculation
   - Weight redistribution maintains relative dimension importance
   - Scores remain stable for 500+ word documents (regression prevention)

---

## Related Stories

- **Story 1.4.5**: Advanced Lexical Dimension - Refactored dimension that this story enhances
- **Story 2.4**: Dimension Scoring Optimization - May benefit from adaptive weighting system
- **Story 2.5**: Percentile-Anchored Scoring - Thresholds may need adjustment for tiered analysis

---

## Migration Guide

### For Users

**Before (Current Behavior)**:
```bash
$ python analyze_ai_patterns.py application.txt --mode full --profile full

Advanced Lexical (Richness): POOR          (HDD: N/A, Yule's K: N/A)
Quality Score: 80.9 / 100  Gap: +4.1 pts
Top Actions: Analyzes advanced lexical diversity → +10.5 pts
```

**After (v5.3.0)**:
```bash
$ python analyze_ai_patterns.py application.txt --mode full --profile full

SHORT CONTENT MODE (Document: 317 words)
- Using 10 dimensions with adaptive scoring (76% coverage)
- Advanced Lexical metrics: ESTIMATED (TTR-based approximation)

Advanced Lexical (Richness): GOOD          (HDD: 0.73*, Yule's K: 45.2*)
  * ESTIMATED - TTR-based approximation for short text (<500 words)

Quality Score: 85.3 / 100  Gap: +0.0 pts (TARGET MET)
```

### For Developers

**Breaking Changes**: None (backward compatible)

**New Fields**:
- `AnalysisResults.analysis_tier`: Enum value (MICRO/SHORT/STANDARD)
- `DimensionResult.short_text_mode`: Boolean flag
- `DimensionResult.reliability`: String ('EXACT', 'ESTIMATED', 'APPROXIMATED')

**Configuration**:
```python
# Opt out of adaptive scoring (force standard behavior)
config = AnalysisConfig(
    mode=AnalysisMode.FULL,
    adaptive_scoring=False  # New option (default: True)
)
```

---

## Notes

### Research Basis

1. **TTR-HDD Correlation**: Research shows TTR correlates 0.85+ with HDD for documents >100 words (McCarthy & Jarvis, 2010)
2. **MATTR Segment Sizing**: Optimal segment = 1/3 to 1/2 of document length (Koizumi & In'nami, 2012)
3. **Short Content Thresholds**: 500 words is standard minimum for lexical diversity research (Crossley et al., 2011)

### Alternative Approaches Considered

**Rejected**: Require 500+ words minimum
- **Rationale**: Too restrictive for common use cases (emails, social posts, paragraphs)

**Rejected**: Always show N/A for unavailable metrics
- **Rationale**: Poor UX, misleading quality gaps, limits tool utility

**Selected**: Adaptive scoring with tier-based fallbacks
- **Rationale**: Best balance of accuracy, usability, and transparency

### Future Enhancements

- **Story 2.8**: Machine learning model for HDD/Yule's K prediction (trained on 10k+ documents)
- **Story 2.9**: Corpus-relative scoring (compare to similar-length documents)
- **Story 2.10**: Multi-document analysis (aggregate scores across related short documents)

---

## Approval

**Author**: AI Pattern Analyzer Team
**Reviewer**: [Pending]
**Status**: PROPOSED
**Last Updated**: 2025-11-19
