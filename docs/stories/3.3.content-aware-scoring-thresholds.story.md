# Story 3.3: Content-Aware Scoring Thresholds

**Status:** ✅ APPROVED
**Parent Epic:** Content-Aware Analysis System (Epic 3.0)
**Estimated Effort:** 2-3 days
**Dependencies:** Story 3.1 (Content Type Detection), Story 3.2 (Content-Aware Weighting)
**Priority:** HIGH (Completes content-aware scoring system)
**Target Version:** 1.5.0

---

## Story

As a **technical writer analyzing different content types**,
I want **EXCELLENT/GOOD/POOR thresholds to adjust based on content type**,
so that **the same metric value is scored appropriately for different genres** (e.g., Flesch 55 = EXCELLENT for technical books, POOR for blogs).

---

## Acceptance Criteria

### AC1: CONTENT_TYPE_THRESHOLDS Configuration
- [ ] Define `CONTENT_TYPE_THRESHOLDS` dict in `content_type_config.py`
- [ ] Includes scoring curves for all 9 content types
- [ ] Each dimension specifies EXCELLENT, GOOD, POOR ranges by content type
- [ ] Ranges are tuples: `(min, max)` for continuous metrics
- [ ] Thresholds reflect genre conventions from CONTENT-TYPE-DIMENSION-MAPPING.md

### AC2: Threshold Examples - Readability (Flesch Reading Ease)
- [ ] **Academic**: EXCELLENT (20-50), GOOD (15-55), POOR (60-100)
  - Formal complexity expected, accessibility inappropriate
- [ ] **Technical Book**: EXCELLENT (55-70), GOOD (50-75), POOR (0-45 or 80-100)
  - Must be accessible for learning, not too simple or complex
- [ ] **Blog**: EXCELLENT (65-80), GOOD (60-85), POOR (0-55)
  - Conversational accessibility required, complexity penalized
- [ ] **Professional Bio**: EXCELLENT (60-70), GOOD (55-75), POOR (0-50 or 80-100)
  - Clear but professional tone

### AC3: Threshold Examples - Sentiment Variance
- [ ] **Professional Bio**: EXCELLENT (0.000-0.030), GOOD (0.000-0.050), POOR (0.200-0.500)
  - Factual neutrality expected, emotional variation inappropriate
- [ ] **Personal Statement**: EXCELLENT (0.200-0.400), GOOD (0.150-0.450), POOR (0.000-0.050)
  - Emotional variation required, flatness penalized
- [ ] **Technical Book**: EXCELLENT (0.100-0.250), GOOD (0.050-0.300), POOR (0.000-0.030)
  - Appropriate enthusiasm, not flat or overly emotional
- [ ] **Academic**: EXCELLENT (0.000-0.050), GOOD (0.000-0.100), POOR (0.250-0.500)
  - Objective tone expected

### AC4: Threshold Examples - Burstiness (Sentence StdDev)
- [ ] **Creative**: EXCELLENT (10-15), GOOD (8-16), POOR (0-5)
  - Highly varied rhythm required
- [ ] **Technical Docs**: EXCELLENT (4-6), GOOD (3-7), POOR (10-20)
  - Uniform structure preferred for scanability
- [ ] **Technical Book**: EXCELLENT (6-10), GOOD (5-11), POOR (0-4)
  - Moderate variation for engagement over 300 pages
- [ ] **Blog**: EXCELLENT (7-12), GOOD (6-13), POOR (0-4)
  - Punchy variation expected

### AC5: DimensionStrategy Integration
- [ ] Each dimension's `score_metric()` method accepts `content_type: Optional[str]`
- [ ] Dimensions query thresholds: `get_content_thresholds(content_type, dimension, metric)`
- [ ] Returns appropriate EXCELLENT/GOOD/POOR assessment based on content type
- [ ] Falls back to generic thresholds if content_type=None

### AC6: Dimension Score Assessment
- [ ] All dimensions use content-aware thresholds when content_type provided
- [ ] Assessment logic: metric value → (EXCELLENT|GOOD|NEEDS WORK|POOR) by content type
- [ ] Report shows assessments appropriate for genre:
  - `Readability: EXCELLENT (Flesch: 55)` for technical_book
  - `Readability: POOR (Flesch: 55)` for blog (same metric, different assessment)

### AC7: Testing Coverage
- [ ] Unit tests achieve ≥85% coverage for content threshold module
- [ ] Test all threshold ranges are valid (min < max)
- [ ] Test same metric value produces different assessments by content type
- [ ] Test fallback to generic thresholds when content_type=None
- [ ] Test boundary cases (metric exactly at threshold boundary)

---

## Tasks/Subtasks

### Task 1: CONTENT_TYPE_THRESHOLDS Configuration (1.5 days)
- [ ] Extend `content_type_config.py` with CONTENT_TYPE_THRESHOLDS
- [ ] Define threshold ranges for **Readability** (9 content types)
- [ ] Define threshold ranges for **Sentiment** (9 content types)
- [ ] Define threshold ranges for **Burstiness** (9 content types)
- [ ] Define threshold ranges for **Perplexity** (AI word density)
- [ ] Define threshold ranges for **Advanced Lexical** (HDD, Yule's K)
- [ ] Define threshold ranges for **Figurative Language** (density per 1k)
- [ ] Define threshold ranges for other dimensions (Voice, Structure, etc.)

### Task 2: Threshold Accessor Function (0.25 days)
- [ ] Implement `get_content_thresholds(content_type, dimension, metric) -> Dict[str, Tuple]`
- [ ] Returns `{'EXCELLENT': (min, max), 'GOOD': (min, max), 'POOR': (min, max)}`
- [ ] Falls back to generic thresholds if content_type unknown or None
- [ ] Validates threshold ranges (min < max, no overlaps)

### Task 3: DimensionStrategy Integration (0.5 days)
- [ ] Add `content_type: Optional[str]` parameter to `score_metric()` in BaseDimensionStrategy
- [ ] Update all dimension strategies to accept content_type
- [ ] Query content-specific thresholds in assessment logic
- [ ] Pass content_type through analyzer pipeline to dimensions

### Task 4: Update Individual Dimensions (0.5 days)
- [ ] Update ReadabilityDimension to use content thresholds
- [ ] Update SentimentDimension to use content thresholds
- [ ] Update BurstinessDimension to use content thresholds
- [ ] Update PerplexityDimension to use content thresholds
- [ ] Update AdvancedLexicalDimension to use content thresholds
- [ ] Update FigurativeLanguageDimension to use content thresholds
- [ ] Update remaining dimensions as applicable

### Task 5: Testing (0.5 days)
- [ ] Unit tests for threshold range validation
- [ ] Unit tests for same metric, different assessment by content type
- [ ] Integration tests with sample documents for each content type
- [ ] Test fallback to generic thresholds
- [ ] Test boundary cases (metric at exact threshold)

### Task 6: Documentation (0.25 days)
- [ ] Update CHANGELOG.md with Story 3.3 changes
- [ ] Document content-aware threshold examples in README
- [ ] Add usage examples showing different assessments
- [ ] Document threshold configuration format

---

## Dev Notes

### Background Context

**Problem**: Current analyzer uses uniform EXCELLENT/GOOD/POOR thresholds regardless of content type. This causes inappropriate assessments:

**Example 1 - Readability (Flesch Reading Ease 55)**:
- Current: POOR (assumes all content should be highly accessible)
- Academic: Should be POOR (too simple for scholarly work, target 20-50)
- Technical Book: Should be EXCELLENT (perfect learning level, target 55-70)
- Blog: Should be POOR (too complex for conversational content, target 65-80)

**Example 2 - Sentiment Variance (0.000)**:
- Current: POOR (assumes all content should have emotional variation)
- Professional Bio: Should be EXCELLENT (factual neutrality expected)
- Personal Statement: Should be POOR (emotional variation required)
- Academic: Should be EXCELLENT (objective tone expected)

**Solution**: Content-aware scoring thresholds adjust assessment criteria based on genre conventions. The same metric value produces different assessments depending on content type.

### Key Insight: Weight vs Threshold

**Story 3.2 (Weight)**: Adjusts dimension *importance* for quality score
- Technical books: readability weighted 15% (high importance)
- Blogs: readability weighted 10% (medium importance)

**Story 3.3 (Threshold)**: Adjusts dimension *assessment* criteria
- Technical books: Flesch 55 = EXCELLENT (appropriate for learning)
- Blogs: Flesch 55 = POOR (too complex for conversational tone)

**Combined Effect**:
```
Technical Book with Flesch 55:
  - Assessment: EXCELLENT (content-aware threshold)
  - Weight: 15% (content-aware weighting)
  - Contribution: EXCELLENT × 15% = strong positive

Blog with Flesch 55:
  - Assessment: POOR (content-aware threshold)
  - Weight: 10% (content-aware weighting)
  - Contribution: POOR × 10% = moderate negative
```

### Implementation Strategy

**1. CONTENT_TYPE_THRESHOLDS Configuration**
```python
# writescore/core/content_type_config.py (EXTEND)

CONTENT_TYPE_THRESHOLDS = {
    'technical_book': {
        'readability': {
            'flesch_reading_ease': {
                'EXCELLENT': (55, 70),   # Accessible for learning
                'GOOD': (50, 75),
                'NEEDS_WORK': (45, 50),  # Getting too complex
                'POOR': (0, 45),         # Too complex for learners
            },
            'flesch_kincaid_grade': {
                'EXCELLENT': (9, 12),    # High school to college freshman
                'GOOD': (8, 13),
                'NEEDS_WORK': (13, 14),
                'POOR': (14, 20),        # Graduate level inappropriate
            }
        },
        'sentiment': {
            'variance': {
                'EXCELLENT': (0.100, 0.250),  # Appropriate enthusiasm
                'GOOD': (0.050, 0.300),
                'NEEDS_WORK': (0.030, 0.050),
                'POOR': (0.000, 0.030),       # Too dry for 300 pages
            }
        },
        'burstiness': {
            'stdev': {
                'EXCELLENT': (6, 10),    # Varied for engagement
                'GOOD': (5, 11),
                'NEEDS_WORK': (4, 5),
                'POOR': (0, 4),          # Too uniform/boring
            }
        },
        'perplexity': {
            'ai_word_density': {
                'EXCELLENT': (0, 1),     # Minimal AI vocabulary
                'GOOD': (1, 3),
                'NEEDS_WORK': (3, 5),
                'POOR': (5, 50),         # AI-like patterns
            }
        },
    },
    'academic': {
        'readability': {
            'flesch_reading_ease': {
                'EXCELLENT': (20, 50),   # Formal complexity expected
                'GOOD': (15, 55),
                'NEEDS_WORK': (55, 60),
                'POOR': (60, 100),       # Too accessible/informal
            },
            'flesch_kincaid_grade': {
                'EXCELLENT': (15, 20),   # Graduate level
                'GOOD': (13, 20),
                'NEEDS_WORK': (12, 13),
                'POOR': (0, 12),         # Too simple for scholarship
            }
        },
        'sentiment': {
            'variance': {
                'EXCELLENT': (0.000, 0.050),  # Objective tone
                'GOOD': (0.000, 0.100),
                'NEEDS_WORK': (0.100, 0.150),
                'POOR': (0.200, 0.500),       # Emotional bias
            }
        },
        'burstiness': {
            'stdev': {
                'EXCELLENT': (5, 8),     # Moderate variation
                'GOOD': (4, 9),
                'NEEDS_WORK': (3, 4),
                'POOR': (0, 3),          # Too uniform
            }
        },
    },
    'blog': {
        'readability': {
            'flesch_reading_ease': {
                'EXCELLENT': (65, 80),   # Conversational accessibility
                'GOOD': (60, 85),
                'NEEDS_WORK': (55, 60),
                'POOR': (0, 55),         # Too complex
            },
            'flesch_kincaid_grade': {
                'EXCELLENT': (6, 8),     # Middle school reading level
                'GOOD': (5, 9),
                'NEEDS_WORK': (9, 10),
                'POOR': (10, 20),        # Too formal/complex
            }
        },
        'sentiment': {
            'variance': {
                'EXCELLENT': (0.150, 0.350),  # Engaging variation
                'GOOD': (0.100, 0.400),
                'NEEDS_WORK': (0.050, 0.100),
                'POOR': (0.000, 0.050),       # Too flat/boring
            }
        },
        'burstiness': {
            'stdev': {
                'EXCELLENT': (7, 12),    # Punchy variation
                'GOOD': (6, 13),
                'NEEDS_WORK': (5, 6),
                'POOR': (0, 5),          # Too uniform
            }
        },
    },
    'professional_bio': {
        'readability': {
            'flesch_reading_ease': {
                'EXCELLENT': (60, 70),   # Clear professional tone
                'GOOD': (55, 75),
                'NEEDS_WORK': (50, 55),
                'POOR': (0, 50),         # Too complex
            },
        },
        'sentiment': {
            'variance': {
                'EXCELLENT': (0.000, 0.030),  # Factual neutrality
                'GOOD': (0.000, 0.050),
                'NEEDS_WORK': (0.050, 0.100),
                'POOR': (0.200, 0.500),       # Emotional bias inappropriate
            }
        },
        'burstiness': {
            'stdev': {
                'EXCELLENT': (6, 10),    # Professional variation
                'GOOD': (5, 11),
                'NEEDS_WORK': (4, 5),
                'POOR': (0, 4),
            }
        },
    },
    'personal_statement': {
        'readability': {
            'flesch_reading_ease': {
                'EXCELLENT': (50, 70),   # Accessible narrative
                'GOOD': (45, 75),
                'NEEDS_WORK': (40, 45),
                'POOR': (0, 40),
            },
        },
        'sentiment': {
            'variance': {
                'EXCELLENT': (0.200, 0.400),  # Emotional depth required
                'GOOD': (0.150, 0.450),
                'NEEDS_WORK': (0.100, 0.150),
                'POOR': (0.000, 0.050),       # Too flat/robotic
            }
        },
        'burstiness': {
            'stdev': {
                'EXCELLENT': (8, 13),    # Narrative rhythm
                'GOOD': (7, 14),
                'NEEDS_WORK': (6, 7),
                'POOR': (0, 6),
            }
        },
    },
    'technical_docs': {
        'readability': {
            'flesch_reading_ease': {
                'EXCELLENT': (60, 70),   # Scannable clarity
                'GOOD': (55, 75),
                'NEEDS_WORK': (50, 55),
                'POOR': (0, 50),
            },
        },
        'sentiment': {
            'variance': {
                'EXCELLENT': (0.000, 0.030),  # Neutral instructions
                'GOOD': (0.000, 0.050),
                'NEEDS_WORK': (0.050, 0.100),
                'POOR': (0.150, 0.500),
            }
        },
        'burstiness': {
            'stdev': {
                'EXCELLENT': (4, 6),     # Uniform for scanability
                'GOOD': (3, 7),
                'NEEDS_WORK': (7, 8),
                'POOR': (10, 20),        # Too varied/distracting
            }
        },
        'structure': {
            'formulaic_transitions': {
                'EXCELLENT': (0, 5),     # Formulaic OK for findability
                'GOOD': (0, 8),
                'NEEDS_WORK': (8, 10),
                'POOR': (10, 50),
            }
        },
    },
    # ... business, creative, news (similar structure)
}

def get_content_thresholds(
    content_type: str,
    dimension: str,
    metric: str
) -> Optional[Dict[str, Tuple[float, float]]]:
    """Get content-specific thresholds for dimension metric"""
    if content_type not in CONTENT_TYPE_THRESHOLDS:
        return None  # Falls back to generic thresholds

    dimension_thresholds = CONTENT_TYPE_THRESHOLDS[content_type].get(dimension)
    if not dimension_thresholds:
        return None

    metric_thresholds = dimension_thresholds.get(metric)
    return metric_thresholds
```

**2. DimensionStrategy Integration**
```python
# writescore/dimensions/base_strategy.py (MODIFY)

class BaseDimensionStrategy(ABC):
    @abstractmethod
    def score_metric(
        self,
        text: str,
        content_type: Optional[str] = None
    ) -> DimensionScore:
        """Score dimension with content-aware thresholds"""
        pass

    def assess_metric(
        self,
        metric_value: float,
        metric_name: str,
        content_type: Optional[str] = None
    ) -> str:
        """Return EXCELLENT/GOOD/NEEDS_WORK/POOR assessment"""
        thresholds = get_content_thresholds(
            content_type,
            self.dimension_name,
            metric_name
        )

        if thresholds is None:
            # Fallback to generic thresholds
            thresholds = self._get_default_thresholds(metric_name)

        # Apply threshold logic
        if self._in_range(metric_value, thresholds['EXCELLENT']):
            return 'EXCELLENT'
        elif self._in_range(metric_value, thresholds['GOOD']):
            return 'GOOD'
        elif self._in_range(metric_value, thresholds.get('NEEDS_WORK', (float('-inf'), float('-inf')))):
            return 'NEEDS_WORK'
        else:
            return 'POOR'

    def _in_range(self, value: float, range_tuple: Tuple[float, float]) -> bool:
        """Check if value falls within range (inclusive)"""
        return range_tuple[0] <= value <= range_tuple[1]
```

**3. ReadabilityDimension Example**
```python
# writescore/dimensions/readability.py (MODIFY)

class ReadabilityDimension(BaseDimensionStrategy):
    def score_metric(
        self,
        text: str,
        content_type: Optional[str] = None
    ) -> DimensionScore:
        flesch_ease = textstat.flesch_reading_ease(text)
        fk_grade = textstat.flesch_kincaid_grade(text)

        # Content-aware assessment
        ease_assessment = self.assess_metric(
            flesch_ease,
            'flesch_reading_ease',
            content_type
        )

        grade_assessment = self.assess_metric(
            fk_grade,
            'flesch_kincaid_grade',
            content_type
        )

        # Overall assessment (weighted average or worst-case)
        overall = self._combine_assessments(ease_assessment, grade_assessment)

        return DimensionScore(
            assessment=overall,
            details=f"Flesch: {flesch_ease:.1f}, Grade: {fk_grade:.1f}",
            metrics={
                'flesch_reading_ease': flesch_ease,
                'flesch_kincaid_grade': fk_grade,
            }
        )
```

**4. Example Output Comparison**

**Before (Generic Thresholds)**:
```
Analyzing: chapter-01.md (technical book)

Readability:   POOR    (Flesch: 55.2, Grade: 11.3)
Sentiment:     POOR    (Variance: 0.150)
Burstiness:    GOOD    (StdDev: 7.8)

Quality Score: 58/100
```

**After (Content-Aware Thresholds)**:
```
Analyzing: chapter-01.md
Content Type: Technical Book (detected, confidence: 0.87)

Readability (15%):   EXCELLENT  (Flesch: 55.2, Grade: 11.3)
  ✓ Perfect accessibility for learning content
Sentiment (8%):      EXCELLENT  (Variance: 0.150)
  ✓ Appropriate enthusiasm for 300-page book
Burstiness (12%):    GOOD       (StdDev: 7.8)
  ✓ Varied for engagement

Quality Score: 89/100 (content-aware scoring)
```

### Relevant Source Tree

```
writescore/
├── core/
│   ├── content_type_config.py         # [EXTEND] Add CONTENT_TYPE_THRESHOLDS
│   └── analyzer.py                    # [MODIFY] Pass content_type to dimensions
├── dimensions/
│   ├── base_strategy.py               # [MODIFY] Add content_type to score_metric()
│   ├── readability.py                 # [MODIFY] Use content thresholds
│   ├── sentiment.py                   # [MODIFY] Use content thresholds
│   ├── burstiness.py                  # [MODIFY] Use content thresholds
│   ├── perplexity.py                  # [MODIFY] Use content thresholds
│   ├── advanced_lexical.py            # [MODIFY] Use content thresholds
│   └── figurative_language.py         # [MODIFY] Use content thresholds
└── tests/
    └── unit/
        └── core/
            └── test_content_thresholds.py  # [NEW] Threshold tests
```

### Performance Requirements

- **Threshold Lookup**: <0.001s per dimension
- **Assessment Calculation**: No measurable overhead vs generic thresholds
- **Memory**: <200KB for threshold configuration storage
- **Validation**: All threshold ranges validated on module load

### Testing Requirements

**Unit Tests** (≥85% coverage):
```python
def test_threshold_ranges_valid():
    """All threshold ranges must have min < max"""
    for content_type, dimensions in CONTENT_TYPE_THRESHOLDS.items():
        for dimension, metrics in dimensions.items():
            for metric, thresholds in metrics.items():
                for assessment, (min_val, max_val) in thresholds.items():
                    assert min_val < max_val, \
                        f"{content_type}.{dimension}.{metric}.{assessment} invalid range"

def test_same_metric_different_assessment_by_content():
    """Flesch 55 should be EXCELLENT for technical_book, POOR for blog"""
    flesch_55 = 55.0

    # Technical book: EXCELLENT (accessible for learning)
    tech_thresholds = get_content_thresholds('technical_book', 'readability', 'flesch_reading_ease')
    assert 55.0 in range(*tech_thresholds['EXCELLENT'])

    # Blog: POOR (too complex for conversational)
    blog_thresholds = get_content_thresholds('blog', 'readability', 'flesch_reading_ease')
    assert 55.0 < blog_thresholds['EXCELLENT'][0]  # Below EXCELLENT range = POOR

def test_sentiment_variance_zero_professional_bio():
    """Sentiment variance 0.000 should be EXCELLENT for professional bio"""
    variance_zero = 0.000

    bio_thresholds = get_content_thresholds('professional_bio', 'sentiment', 'variance')
    assert 0.000 in range(*bio_thresholds['EXCELLENT'])

def test_fallback_to_generic_thresholds():
    """Unknown content type should return None, triggering fallback"""
    thresholds = get_content_thresholds('unknown_type', 'readability', 'flesch_reading_ease')
    assert thresholds is None
```

**Integration Tests**:
```python
def test_technical_book_content_aware_scoring():
    """Technical book with Flesch 55 scores EXCELLENT"""
    text = """
    # Chapter 1: Introduction

    Let me show you how neural networks work. We'll start with the basics...
    """  # Flesch ~55

    config = AnalysisConfig(content_type='technical_book')
    results = analyzer.analyze(text, config)

    assert results.dimension_scores['readability'].assessment == 'EXCELLENT'

def test_blog_same_flesch_scores_poor():
    """Blog with Flesch 55 scores POOR (too complex)"""
    text = """Same text as above"""  # Flesch ~55

    config = AnalysisConfig(content_type='blog')
    results = analyzer.analyze(text, config)

    assert results.dimension_scores['readability'].assessment == 'POOR'
```

### Documentation Requirements

**CHANGELOG.md**:
```markdown
## [1.5.0] - 2025-XX-XX

### Added
- **Content-Aware Scoring Thresholds** (Story 3.3): Same metric values now assessed differently by content type
  - CONTENT_TYPE_THRESHOLDS configuration with genre-specific EXCELLENT/GOOD/POOR ranges
  - Example: Flesch 55 = EXCELLENT for technical books (learning level), POOR for blogs (too complex)
  - Example: Sentiment variance 0.000 = EXCELLENT for professional bios (neutrality), POOR for personal statements (flatness)
  - All dimensions use content-aware assessments when content_type provided
  - Falls back to generic thresholds if content_type unknown
  - Completes content-aware scoring system (3.1 detection + 3.2 weighting + 3.3 thresholds)
```

**README.md**:
```markdown
## Content-Aware Scoring Thresholds

The analyzer adjusts assessment criteria based on content type:

```bash
# Technical book analysis
python analyze_ai_patterns.py chapter-01.md --content-type technical_book

# Output:
# Readability: EXCELLENT (Flesch: 55.2)  ← Accessible for learning

# Blog analysis (same Flesch score)
python analyze_ai_patterns.py blog-post.md --content-type blog

# Output:
# Readability: POOR (Flesch: 55.2)  ← Too complex for conversational content
```

**Threshold Examples**:

| Metric | Value | Academic | Technical Book | Blog |
|--------|-------|----------|----------------|------|
| Flesch Ease | 55 | POOR (too simple) | EXCELLENT (learning level) | POOR (too complex) |
| Sentiment Variance | 0.000 | EXCELLENT (objective) | POOR (too dry) | POOR (too flat) |
| Burstiness σ | 5 | GOOD (moderate) | NEEDS WORK (too uniform) | POOR (too uniform) |
```

---

## QA Results

### Test Coverage
- [ ] Unit tests: ___% (Target: ≥85%)
- [ ] Threshold range validation: 9/9 content types pass
- [ ] Cross-content assessment tests: ___ passing

### Performance Benchmarks
- [ ] Threshold lookup: ___ms (Target: <1ms)
- [ ] Assessment calculation: ___ms (Target: no overhead)
- [ ] Memory overhead: ___KB (Target: <200KB)

### Manual Validation
- [ ] Tested all 9 content type threshold profiles
- [ ] Verified same metric produces different assessments by genre
- [ ] Confirmed fallback to generic thresholds works
- [ ] Validated boundary case handling

---

## Change Log

| Date | Author | Change | Status |
|------|--------|--------|--------|
| 2025-01-XX | jmagady | Initial story creation | APPROVED |
